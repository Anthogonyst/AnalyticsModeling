---
title: "Insurance Quotes"
author: "Anthony Arroyo, Seung min song, Alice A. Friedman"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: 
  pdf_document:
    pandoc_args: --listings
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
(function() { set.seed(8675309) })()
invisible(lapply(list.files(here::here("R"), "\\.R$", full.names = TRUE), source))

library(magrittr)
library(here)
library(reshape2)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(ModelMetrics)
library(readr)
library(tidyr)
library(corrplot)
library(glmnet)

rawData = LoadDataElseWeb("insurance_training_data.csv", "../data/") 
rawTestData = LoadDataElseWeb("insurance-evaluation-data.csv", "../data/")
```

# Abstract

We discuss the challenges from evaluating insurance data, assessing whether a car is involved in a crash and then how expensive the claim would be. Our goal is to create a logistic regression model, which predicts the binary value of whether or not a car has been in a crash (`TARGET_FLAG`), as well as a linear regression, which predicts the amount of the claim (`TARGET_AMT`). Our linear regression models pull some insights regarding the effects of marital status and others on claim costs. MORE TK.

# Part 1. Data Exploration 

The data has 23 features and 2 target variables that we wish to predict. As part of our predictive analytics, first we will decide if the car that belongs to a license plate has been in a crash before. Second, we will determine how much the cost of the crash was if they were.

- TARGET_FLAG: Was car in a crash? 1=YES 0=NO None
- TARGET_AMT: If car was in a crash, what was the cost of the damage?

There are some general myths related to driving that may skew our view about what we expect. How much of it is true? Provided below is a quick data dictionary of the features we will be measuring.

- AGE: Age of Driver
- BLUEBOOK: Value of Vehicle
- CAR_AGE: Vehicle Age
- CAR_TYPE: Type of Car
- CAR_USE: Vehicle Use
- CLM_FREQ: # Claims (Past 5 Years)
- EDUCATION: Max Education Level
- HOMEKIDS: # Children at Home
- HOME_VAL: Home Value
- INCOME: Income
- JOB: Job Category
- KIDSDRIV: # Driving Children
- MSTATUS: Marital Status
- MVR_PTS: Motor Vehicle Record Points
- OLDCLAIM: Total Claims (Past 5 Years)
- PARENT1: Single Parent
- RED_CAR: A Red Car
- REVOKED: License Revoked (Past 7 Years)
- SEX: Gender
- TIF: Time in Force
- TRAVTIME: Distance to Work
- URBANICITY: Home/Work Area
- YOJ: Years on Job

The top of the dataset is shown below, where we can see a smattering of isues such as untidy data and even a missing value in one of the income observations.

```{r head-class}
head(rawData, 4)
#sapply(rawData, class)
```

## Tidy Data

After carefully examining the data in the raw data frame, it was found that certain variables contained unnecessary characters such as dollar signs. To address this issue, regular expression (regex) was used to remove these unwanted characters from the variables. By doing so, the data is now cleaner and more suitable for analysis.

Additionally, there are several nominal categories that are yes and no questions. We reduced this to binary where 1=yes and 0=no using regex again to further simplify the process.

Finally, we convert variables to type factor or numeric, as appropriate.

```{r clean-funcs}
### Akin to dplyr::mutate_at but does not throw error ("silent") on missing cols and applies a function
SilentMutateAt <- function(df, dirtyCols = c("Sample", "Text"), FUN, ...) {
  ind = colnames(df) %in% dirtyCols
  df[, ind] = sapply(df[, ind], FUN = FUN, ... = ...)
  
  return(df)
}

### Rearranges parameters of gsub to be more friendly to pipe operator
GsubPipe <- function(x, pattern, replacement, ...) {
  gsub(pattern, replacement, x, ...)
}

### Replaces "yes/no" questions with an integer where yes=1 and no=0
CleanYesNo <- function(df, trace = FALSE, threshold = 0.5) {
  yesNoCols = sapply(df, function(x) { 
    unique(x) %>% 
      grepl("yes|no|^y$|^n$", ., ignore.case = T) %>% 
        { sum(.) / length(.) } %>%
          { . >= threshold }
  }) %>%
    .[.] %>%
      names(.)
  
  result = SilentMutateAt(df, yesNoCols, function(x) {
    truthy = as.integer(grepl("yes|^y$", x, ignore.case = T))
    truthy = truthy - as.integer(grepl("no|^n$", x, ignore.case = T))
    
    truthy[truthy == 0L] = NA
    truthy[truthy == -1L] = 0L
    
    truthy
  })
  
  if (trace) {
    return(list(result, yesNoCols))
  }
  
  return(result)
}

### Pre-processing data pipeline for insurance data; cleans up strings
CleanInsuranceStrings <- function(df) {
  SilentMutateAt(df, colnames(df)[sapply(df, is.character)], GsubPipe, "\\$|z_|<|,", "") %>% 
    CleanYesNo(.) %>%
      # could also use readr::parse_number here
      SilentMutateAt(c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM"), as.numeric) %>% 
        ### Coerces blank strings to NA
        SilentMutateAt(., "JOB", function(x) { x[x == ""] = "Unspecified" ; x }) %>%
          ### Converts strings to factors
          dplyr::mutate_at(., colnames(.)[sapply(., is.character)], as.factor)
}
```

```{r init}
#runs functions
cleanData <- CleanInsuranceStrings(rawData)
cleanTestData <- CleanInsuranceStrings(rawTestData)
imputeData = CleanInsuranceStrings(rawData)

#reference: https://statisticsglobe.com/convert-character-to-factor-in-r

summary(cleanData)
head(cleanData)
```

## Visulization of the data set

Our features span a multitude of different distributions.  None of them are strictly normal unless `0` values are excluded, although `AGE` comes closest. It is unclear in the data if `0` is equivalent to missing data, or whether some car owners have `0` income (many of whom are coded as students or homemakers) or a `0` home value (perhaps renters?). Several are categorical(e.g., `JOB`), ordinal (e.g., `EDUCATION`) or binomial (e.g., `RED_CAR`). 

```{r categorize}
# list of continuous vars
contVars <- c(
  "TARGET_AMT",
  "AGE",
  "BLUEBOOK",
  "CAR_AGE",
  "HOME_VAL",
  "INCOME",
  "OLDCLAIM",
  "TIF",
  "TRAVTIME",
  "YOJ"
)

# list of categorical, discrete, or binomial vars
factorVars <- c(
  "TARGET_FLAG",
  "CAR_TYPE",
  "CAR_USE",
  "CLM_FREQ",
  "EDUCATION",
  "HOMEKIDS",
  "JOB",
  "KIDSDRIV",
  "MSTATUS",
  "MVR_PTS",
  "PARENT1",
  "RED_CAR",
  "REVOKED",
  "SEX",
  "URBANICITY"
)

```


```{r}

### Plots the distribution of each variable
# See common-helpers.R
nViz <- cleanData %>% tidyr::drop_na()
nViz %>% select(all_of(contVars)) %>%
  select_if(is.numeric) %>%                     # Keep only numeric columns
  gather() %>%                             # Convert to key-value pairs
  ggplot(aes(value)) +                     # Plot the values
    facet_wrap(~ key, scales = "free") +   # In separate panels
    geom_density() 

ggplot(data = nViz %>% select(all_of(factorVars)) %>% gather()) +
  geom_bar(mapping = aes(x = as.factor(value), fill = as.factor(value)), show.legend = FALSE) + 
  facet_wrap( ~ key, scales = "free", ncol = 3)



```

```{r dist}
### Plots the distribution of each variable
# See common-helpers.R
RemoveCols(cleanData, c("INDEX", "TARGET_FLAG", "TARGET_AMT")) %>%
  # See common-stats.R
  PlotVarDistribution(showBoxPlot = TRUE)
```

## Correlation

Several of the continuous variables are show to correlated. We see a strong correlation between `INCOME`, `HOME_VAL`, `BLUEBOOK`, and `CAR_AGE`, which intuitively makes sense as higher income people are more likley to have more expensive houses, and newer, more expensive cars.  

`INCOME` and `HOME_VAL` have the strongest correlation, with a correlation coefficient of 0.58. It is unlikely that both of these features should be used in the final model.


```{r corr}
# Numeric correlational matrix for continuous variables
cleanData %>% select(all_of(contVars)) %>% cor(use = "complete.obs") %>% round(2)

# drill down into variables identified as signifincantly correlated (corr > 0.4)
cleanData %>% select(INCOME, BLUEBOOK, HOME_VAL, CAR_AGE) %>% cor(use = "pairwise.complete.obs") %>% round(2)

# drill down into variables identified as signifincantly correlated (corr > 0.4)
## Income X HOME_VAL
plot(cleanData %>% select(INCOME, BLUEBOOK, HOME_VAL, CAR_AGE))


### Develops a correlation plot for all numeric values
# See common-helpers.R
rawDatanum <- cleanData %>% select(all_of(contVars)) %>% drop_na()

corrplot(cor(rawDatanum), method="square")


```
Categorical data correlations must be calculated differently. Purely categorical data correlation can be assessed using different methods. One method to compare correlation between a continuous and binomial feauture is to determine if a logistic model can be developed with a significant P value.

As shown below, many of the categorical variables are highly correlated. For example, `RED_CAR` is highly predictive of `SEX` == M. Martial status and being a single parent are (of course) mechanistically connected (all single parents are unmarried). 

```{r}
#Define a list of binomial features

binomials <- c(
  "TARGET_FLAG",
  "PARENT1",
  "MSTATUS",
  "RED_CAR",
  "REVOKED",
  "SEX"
)

# relevel M/F as binomial
binomialDF <- cleanData %>% 
  select(all_of(binomials)) %>%
  mutate(
  SEX = SEX %>% plyr::mapvalues(from = c("M", "F"), to = c(0, 1)) %>% as.numeric()
) 


library(ggcorrplot)
model.matrix(~0+., data=binomialDF) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag=FALSE, lab=TRUE, lab_size=2)
```
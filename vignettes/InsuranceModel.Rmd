---
title: "Starter Workbench"
author: "Anthony A"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: 
  pdf_document:
    pandoc_args: --listings
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
(function() { set.seed(8675309) })()
invisible(lapply(list.files(here::here("R"), "\\.R$", full.names = TRUE), source))

library(magrittr)
library(here)
library(reshape2)
library(dplyr)
library(ggplot2)
library(ggrepel)

rawData = LoadDataElseWeb("insurance_training_data.csv", "../data/")
rawTestData = LoadDataElseWeb("insurance-evaluation-data.csv", "../data/")
```

# Abstract
# Data Exploration

```{r}
head(rawData)
sapply(rawData, class)
```

## Tidy Data

After carefully examining the data in the raw data frame, it was found that certain variables contained unnecessary characters such as dollar signs and garbage data. To address this issue, regular expression (regex) was used to remove these unwanted characters from the variables. By doing so, the data is now cleaner and more suitable for analysis.

Additionally, there are several nominal categories that are yes and no questions. We reduced this to factors using regex again to further simplify the process.

```{r clean-funcs}
SilentMutateAt <- function(df, dirtyCols = c("Sample", "Text"), FUN, ...) {
  ind = colnames(df) %in% dirtyCols
  df[, ind] = sapply(df[, ind], FUN = FUN, ... = ...)
  
  df
}

GsubPipe <- function(x, pattern, replacement, ...) {
  gsub(pattern, replacement, x, ...)
}

CleanYesNo <- function(df, trace = FALSE, threshold = 0.5) {
  yesNoCols = sapply(df, function(x) { 
    unique(x) %>% 
      grepl("yes|no|^y$|^n$", ., ignore.case = T) %>% 
        { sum(.) / length(.) } %>%
          { . >= threshold }
  }) %>%
    .[.] %>%
      names(.)
  
  result = SilentMutateAt(df, yesNoCols, function(x) {
    truthy = as.integer(grepl("yes|^y$", x, ignore.case = T))
    truthy = truthy - as.integer(grepl("no|^n$", x, ignore.case = T))
    
    truthy[truthy == 0L] = NA
    truthy[truthy == -1L] = 0L
    
    truthy
  })
  
  if (trace) {
    return(list(result, yesNoCols))
  }
  
  return(result)
}

CleanInsuranceStrings <- function(df) {
  SilentMutateAt(df, colnames(df)[sapply(df, is.character)], GsubPipe, "\\$|z_|<|,", "") %>%
    CleanYesNo(.) %>%
      SilentMutateAt(c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM"), as.numeric) %>%
        SilentMutateAt(., "JOB", function(x) { x[x == ""] = "Unspecified" ; x })
}

cleanData = CleanInsuranceStrings(rawData)
rawData = CleanInsuranceStrings(rawData)
```

## Missing values

The rawData data frame contains 4506 missing values.
```{r}
nrow(rawData[is.na(rawData),])
```

The following variables in the dataset have missing values: **TARGET_FLAG**, **TARGET_AMT**, **AGE**, **YOJ**, and **CAR_AGE**. It is important to address these missing values before proceeding with any analysis.
```{r}
colSums(is.na(rawData))
```
### AGE

Based on the available data, it seems that YOJ tends to increase with age. There is one missing value in the AGE column, which has a corresponding YOJ value of 9. Given the available information, it is reasonable to predict that the missing age value is around 45. Insert the predicted values of AGE 45 into the original dataframe

```{r}
p <-  as.data.frame(9)
colnames(p) <- "YOJ"
```

```{r}
model1 <- lm(AGE ~ YOJ, data = rawData)
predict(model1, newdata = p)
```

```{r}
rawData$AGE <- ifelse(is.na(rawData$AGE), 45, rawData$AGE)
```

```{r}
AGE_df <-rawData %>%
  .[, ! sapply(., is.character) | colnames(.) == "SEX"] %>%
    dplyr::filter(!is.na(YOJ)) %>%
      dplyr::group_by(AGE, SEX) %>%
        dplyr::select(-c('INDEX', 'TARGET_FLAG', 'TARGET_AMT')) %>%
          dplyr::summarize_all(mean)
AGE_df
```

```{r}
AGE_df %>% 
    ggplot(aes(x = AGE, y = YOJ, shape =SEX, colour = SEX)) + 
       stat_smooth(se = TRUE, method = lm) +
        geom_point(aes(x = AGE, y = YOJ, colour = SEX), size = 2.5) +
            ggtitle("Total YOJ vs Age separated with Sex") +
              theme_minimal()

```

### YOJ

Build the linear regression model with AGE as the predictor and YOJ as the response and predict the missing values of YOJ based on the values of AGE. Insert the predicted values of YOJ into the original dataframe.
```{r}
# Subset the data with missing values of YOJ
missing_YOJ <- subset(rawData, is.na(YOJ))

lm_model <- lm(YOJ ~ AGE, data = rawData)

predicted_YOJ <- predict(lm_model, newdata = missing_YOJ)

rounded_YOJ <- round(predicted_YOJ, digits = 0)
rawData$YOJ[is.na(rawData$YOJ)] <- rounded_YOJ


```

### Income

Build the linear regression model with AGE and YOJ as the predictors and INCOME as the response and predict the missing values of INCOME based on the values of AGE and YOJ. insert the predicted values of INCOME into the original dataframe.
```{r}
# Subset the data with missing values of INCOME
missing_INCOME <- subset(rawData, is.na(INCOME))

lm_model <- lm(INCOME ~ AGE + YOJ, data = rawData)

predicted_INCOME <- predict(lm_model, newdata = missing_INCOME)

rounded_INCOME <- round(predicted_INCOME, digits = 0)
rawData[is.na(rawData$INCOME),]$INCOME <- rounded_INCOME

```

### Home-val

Build the linear regression model with AGE, YOJ, and INCOME as predictors and HOME_VAL as the response and predict the missing values of HOME_VAL based on the values of AGE, YOJ, and INCOME. Insert the predicted values of HOME_VAL into the original dataframe
```{r}
# Subset the data with missing values of HOME_VAL
missing_HOME_VAL <- subset(rawData, is.na(HOME_VAL))


lm_model <- lm(HOME_VAL ~ AGE + YOJ + INCOME, data = rawData)


predicted_HOME_VAL <- predict(lm_model, newdata = missing_HOME_VAL)

#rounded_HOME_VAL <- round(predicted_HOME_VAL, digits = 0)
rawData[is.na(rawData$HOME_VAL),]$HOME_VAL <- predicted_HOME_VAL
rawData$HOME_VAL <- round(rawData$HOME_VAL, digits = 0)

head(rawData$HOME_VAL)
```

### Car Age

To handle the missing value in CAR_AGE, we will use chained imputation since it is difficult to assess. This will also fill any other data that is still missing so we now have produced our final cleaning steps. We may next consider doing manipulations such as normalization, zero centering, and other kinds of feature engineering.

```{r}
rawData = rawData %>%
  mice::mice(., method = "pmm", printFlag = FALSE) %>%
    mice::complete(.)
trainClean = rawData
```

## Summary

```{r}
summary(select(rawData, -INDEX))
```

## Box-plot

We have 971 male and 1170 female in this study. Male average income is higher than man. However, man has highest car purchase amount. The boxplots show that the medians and interquartile range of the two distributions are very close.
```{r}
SEX_df <- rawData %>% 
  filter(!is.na(INCOME)) %>%
        group_by(SEX) %>% 
        summarise(INCOME = mean(INCOME))
SEX_df
```

```{r}
ggplot(rawData, aes(x = SEX, y = INCOME, color = SEX, fill = SEX)) +
  geom_bar(data = SEX_df, stat = "identity", alpha = .3) + 
    ggrepel::geom_text_repel(aes(label = HOME_VAL), color = "black", size = 2.5, segment.color = "grey") +
      geom_point() +
        guides(color = "none", fill = "none") +
        theme_bw() +
        labs(
          title = "Income by Sex",
          x = "Sex",
          y = "Income"
        )
```

## Visulization of the data set

Our features span a multitude of different distribtuions. Many of them are normal, others are binomial, and others are nominal. How we handle each feature will thus influence future analysis. 

```{r}
PlotVarDistribution(trainClean[, c(-1, -2, -3)], showBoxPlot = TRUE)
```

## Visualization of Correlation

According to this correlation matrix, all of the features are not strongly correlated to each other. With this in mind, we will go forward and assume that potentially every feature is a principal component and we will not exclude any until further downstream analysis is conducted.

```{r}
PlotCorrEllipse(stats::cor(rawData[, sapply(trainClean, is.numeric)] %>% .[, c(-1, -2, -3)], use = "na.or.complete"))
```

# Data Transformations

For our model, we will explore using power transforms on our model where features A and B can create a derived feature A*B. We have previously verified that there is very low colinearity in our data so this will be safe to use, although it will raise our computation effort.

```{r}
nominalCols = c("KIDSDRIV", "HOMEKIDS", "CLM_FREQ")
binomialCols = c("PARENT1", "MSTATUS", "RED_CAR", "REVOKED")
transformedData = cbind(cleanData, CrossProdFeatures(cleanData, dropCols = c(
  "INDEX", "TARGET_FLAG", "TARGET_AMT", nominalCols, binomialCols
))) %>%
  RemoveCols(., "INDEX")

colnames(transformedData) %>%
  .[! . %in% colnames(trainClean)]
```

# Model Building
# Selecting Best Model
# Conclusions and Suggestions
# References
# Appendix

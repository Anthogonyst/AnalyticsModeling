---
title: "Starter Workbench"
author: "Group 5"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: 
  pdf_document:
    pandoc_args: --listings
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
(function() { set.seed(8675309) })()
# What functions are we pulling from these files? 
invisible(lapply(list.files(here::here("R"), "\\.R$", full.names = TRUE), source))

library(magrittr)
library(here)
library(reshape2)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(readr)


rawData = read_csv("../data/insurance_training_data.csv") 
rawTestData = LoadDataElseWeb("insurance-evaluation-data.csv", "../data/")
```

# Abstract
TK

# Data Exploration & Preparation

```{r}
sapply(rawData, class)
head(rawData)
```

## Tidy Data

After carefully examining the data in the raw data frame, it was found that certain variables contained unnecessary characters such as dollar signs. To address this issue, regular expression (regex) was used to remove these unwanted characters from the variables. By doing so, the data is now cleaner and more suitable for analysis.

Additionally, there are several nominal categories that are yes and no questions. We reduced this to binary where 1=yes and 0=no using regex again to further simplify the process.

Finally, we convert variables to type factor or numeric, as appropriate.

```{r clean-func, eval=FALSE}
#can you explain what this does with comments?
SilentMutateAt <- function(df, dirtyCols = c("Sample", "Text"), FUN, ...) {
  ind = colnames(df) %in% dirtyCols
  df[, ind] = sapply(df[, ind], FUN = FUN, ... = ...)
  
  return(df)
}

#can you explain what this does with comments?is this different that just gsub?
GsubPipe <- function(x, pattern, replacement, ...) {
  gsub(pattern, replacement, x, ...)
}

#This function replaces "yes/no" questions with a dummy variable structure where yes=1 and no=0
CleanYesNo <- function(df, trace = FALSE, threshold = 0.5) {
  yesNoCols = sapply(df, function(x) { 
    unique(x) %>% 
      grepl("yes|no|^y$|^n$", ., ignore.case = T) %>% 
        { sum(.) / length(.) } %>%
          { . >= threshold }
  }) %>%
    .[.] %>%
      names(.)
  
  result = SilentMutateAt(df, yesNoCols, function(x) {
    truthy = as.integer(grepl("yes|^y$", x, ignore.case = T))
    truthy = truthy - as.integer(grepl("no|^n$", x, ignore.case = T))
    
    truthy[truthy == 0L] = NA
    truthy[truthy == -1L] = 0L
    
    truthy
  })
  
  if (trace) {
    return(list(result, yesNoCols))
  }
  
  return(result)
}

CleanInsuranceStrings <- function(df) {
  SilentMutateAt(df, colnames(df)[sapply(df, is.character)], GsubPipe, "\\$|z_|<|,", "") %>% 
    CleanYesNo(.) %>%
    SilentMutateAt(c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM"), as.numeric) %>% #could also use readr::parse_number here
    SilentMutateAt(., "JOB", function(x) { x[x == ""] = NA ; x }) %>% #what does this do?
    unclass() %>%
    as.data.frame(, stringsAsFactors=TRUE) #returns char as factor
    }

#runs functions
cleanData <- CleanInsuranceStrings(rawData) 
TestData <- CleanInsuranceStrings(rawTestData)  
        
#reference: https://statisticsglobe.com/convert-character-to-factor-in-r

#rawData = CleanInsuranceStrings(rawData) #so is this no longer raw then? we shouldn't reuse the term in that case
summary(cleanData)
head(cleanData)
```

## Missing values
The rawData data frame contains 970 missing values, representing 948 out of 8161 observations. Most observations wiht
```{r}
colSums(is.na(rawData)) %>% sum() #total missing values
sum(!complete.cases(rawData)) #observations with missing data
```


```{r clean-func2}
## NOTE TO TEAM: we don't have to use this code, but here is another version that uses more built-in functions
## I also see some advantages to leaving the binary values as factors so we don't predict impossible values

# Define a list of columns formatted as currency
hasDollars <- c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM") 

# Parse the numeric data in those columns to remove excess characters, e.g. $, using readr::parse_numbers
df <- rawData %>% mutate(across(hasDollars, readr::parse_number))


# Next, classify characters as factors and relevel as appropriate
df <- df %>% unclass () %>% as.data.frame(stringsAsFactors = TRUE) %>%
  mutate(
    TARGET_FLAG = as.factor(TARGET_FLAG),                         # Set target as factor
    RED_CAR = forcats::fct_recode(RED_CAR,"Yes"="yes", "No"="no") # Clean up case
  ) 

## Define a function to remove "<" or any other non-alphanumeric prefix from levels
remove_non_alphanum_prefix <- function(x) {
  levels(x) <- gsub("^[^[:alnum:]]+",   # This removes any non alphanumeric character from the beginning of a factor level
                    "",                 # and replaces it with an empty string
                    levels(x))          # given a vector set as factor 
  return(x)                             # and then returns the feature.        
}

## Define a function to remove "z_" or "Z_" prefix from levels

remove_z_prefix <- function(x) {
  levels(x) <- gsub("z_",               # This removes any "^z_" from the beginning of a factor level
                    "",                 # and replaces it with an empty string
                    levels(x),        
                    ignore.case = TRUE  # ignoring case        
                    ) 
  return(x)                             # and then returns the feature.        
}


## Apply the functions to all columns of the data frame
df <- as.data.frame(lapply(df, remove_non_alphanum_prefix))
df <- as.data.frame(lapply(df, remove_z_prefix))

# Summarize
summary(df)

# Rename data set as used elsewhere in the code
cleanData <- df
```

## Missing values
The rawData data frame contains 2405 missing values, representing 2116 out of 8161 observations. This is too many to simply drop all rows with missing data; however -- 'AGE' has only 6 missing variables so in that case the most straightforward approach is to drop those rows.

```{r}
print("Total missing values")
colSums(is.na(cleanData)) %>% sum() #total missing values
print("Observations with missing values")
sum(!complete.cases(cleanData)) #observations with missing data
("Number of missing values by feature")
colSums(is.na(cleanData))
```
A good rule of thumb is to drop any feature with more than 80% missing values -- that is not the case with any features here. The remaining options are to:
1) Drop rows with missing values
2) Infer missing values

In order to decide which course to take we should first determine if there is any correlation between features with missing values, or if values are missing at random. Because several of these features are numeric, we will first need to convert the features to the appropriate data type in order to determine if there is a correlation.

The following variables in the data set have missing values: **AGE** (6), **YOJ** (454), **INCOME** (445), **HOME_VAL** (464) and **CAR_AGE**(510). 

First, we will drop the missng **AGE** rows, then we will test each remaining variable for correlation with the other features in the data set to determin how best to handle.


```{r}
df <- cleanData %>% tidyr::drop_na(AGE)
```



```{r, warning=FALSE}
# Define list of columns to test
cols <- c("YOJ", "HOME_VAL", "INCOME", "CAR_AGE")


# Define a function to test correlation with missingness and other features
MAR_test <- function(df, cols) {
  
  # Define a function to relevel by missing values
  relevel_missing <- function(x) {
    for (i in 1:length(x)) {
      x[i] <- dplyr::if_else(is.na(x[i]), 1, 0)
      }
    return(x)
    }


  for (col in cols){
    # Create a var to hold the releveled values
    is_missing <- relevel_missing(subset(df, select=col)) %>% unlist()
    print(class(is_missing))
    
    # p values we will fill in
    p_vals <- rep(NA, ncol(df)) 

    # Loop through every remaining column
    for (j in 1:(ncol(df))) {
	    s <- summary (glm(is_missing ~ df[, j], family = binomial))
	    p_vals[j] <- s$coefficients[2,4]
    }

    results <- cbind(names(df), p_vals) %>% subset(p_vals < 0.05)
    
    print(col)
    print(results)
  }
}



MAR_test(df, cols)

```

All the remaining features with missing values have correlation with at least one other feautre, except **CAR_AGE**. **CAR_AGE** can be said to plausibly be missing completely at random, and so those rows can also be dropped without any expectation of affecting the model performane.

```{r drop_missing_car_age}
df <- cleanData %>% tidyr::drop_na(AGE, CAR_AGE)
```
The remaining features will require imputed values.


### YOJ

Build the linear regression model with AGE as the predictor and YOJ as the response and predict the missing values of YOJ based on the values of AGE. Insert the predicted values of YOJ into the original dataframe.
```{r}
# Subset the data with missing values of YOJ
missing_YOJ <- subset(rawData, is.na(YOJ))

lm_model <- lm(YOJ ~ AGE, data = rawData)

predicted_YOJ <- predict(lm_model, newdata = missing_YOJ)

rounded_YOJ <- round(predicted_YOJ, digits = 0)
rawData$YOJ[is.na(rawData$YOJ)] <- rounded_YOJ


```

### Income

Build the linear regression model with AGE and YOJ [why?] as the predictors and INCOME as the response and predict the missing values of INCOME based on the values of AGE and YOJ. insert the predicted values of INCOME into the original dataframe.
```{r}
# Subset the data with missing values of INCOME
missing_INCOME <- subset(rawData, is.na(INCOME))

lm_model <- lm(INCOME ~ AGE + YOJ, data = rawData)

predicted_INCOME <- predict(lm_model, newdata = missing_INCOME)

rounded_INCOME <- round(predicted_INCOME, digits = 0)
rawData[is.na(rawData$INCOME),]$INCOME <- rounded_INCOME

```

### Home-val

Build the linear regression model with AGE, YOJ, and INCOME as predictors and HOME_VAL as the response and predict the missing values of HOME_VAL based on the values of AGE, YOJ, and INCOME. Insert the predicted values of HOME_VAL into the original dataframe
```{r}
# Subset the data with missing values of HOME_VAL
missing_HOME_VAL <- subset(rawData, is.na(HOME_VAL))


lm_model <- lm(HOME_VAL ~ AGE + YOJ + INCOME, data = rawData)


predicted_HOME_VAL <- predict(lm_model, newdata = missing_HOME_VAL)

#rounded_HOME_VAL <- round(predicted_HOME_VAL, digits = 0)
rawData[is.na(rawData$HOME_VAL),]$HOME_VAL <- predicted_HOME_VAL
rawData$HOME_VAL <- round(rawData$HOME_VAL, digits = 0)

head(rawData$HOME_VAL)
```

### Car Age

To handle the missing value in CAR_AGE, we will use chained imputation since it is difficult to assess. This will also fill any other data that is still missing so we now have produced our final cleaning steps. We may next consider doing manipulations such as normalization, zero centering, and other kinds of feature engineering.

```{r}
rawData = rawData %>%
  mice::mice(., method = "pmm", printFlag = FALSE) %>%
    mice::complete(.)
trainClean = rawData
```

## Summary

```{r}
summary(select(rawData, -INDEX))
```

## Box-plot

We have 971 male and 1170 female in this study. Male average income is higher than man. However, man has highest car purchase amount. The boxplots show that the medians and interquartile range of the two distributions are very close.
```{r}
SEX_df <- rawData %>% 
  filter(!is.na(INCOME)) %>%
        group_by(SEX) %>% 
        summarise(INCOME = mean(INCOME))
SEX_df
```

```{r}
ggplot(rawData, aes(x = SEX, y = INCOME, color = SEX, fill = SEX)) +
  geom_bar(data = SEX_df, stat = "identity", alpha = .3) + 
    ggrepel::geom_text_repel(aes(label = HOME_VAL), color = "black", size = 2.5, segment.color = "grey") +
      geom_point() +
        guides(color = "none", fill = "none") +
        theme_bw() +
        labs(
          title = "Income by Sex",
          x = "Sex",
          y = "Income"
        )
```

## Visulization of the data set

Our features span a multitude of different distributions. Many of them are normal, others are binomial, and others are nominal. How we handle each feature will thus influence future analysis. 

```{r}
PlotVarDistribution(trainClean[, c(-1, -2, -3)], showBoxPlot = TRUE)
```

## Visualization of Correlation

According to this correlation matrix, all of the features are not strongly correlated to each other. With this in mind, we will go forward and assume that potentially every feature is a principal component and we will not exclude any until further downstream analysis is conducted.

```{r}
PlotCorrEllipse(stats::cor(cleanData[, sapply(cleanData, is.numeric)] %>% .[, c(-1, -2, -3)], use = "na.or.complete"))
```

# Data Transformations

For our model, we will explore using power transforms on our model where features A and B can create a derived feature A*B. We have previously verified that there is very low colinearity in our data so this will be safe to use, although it will raise our computation effort.

```{r}
nominalCols = c("KIDSDRIV", "HOMEKIDS", "CLM_FREQ")
binomialCols = c("PARENT1", "MSTATUS", "RED_CAR", "REVOKED")
transformedData = cbind(trainClean, CrossProdFeatures(trainClean, dropCols = c(
  "INDEX", "TARGET_FLAG", "TARGET_AMT", nominalCols, binomialCols
)))

colnames(transformedData) %>%
  .[! . %in% colnames(trainClean)]
```
Remove dollar sign ($) from columns and change to numeric
```{r}
rawTrainData$INCOME <- gsub("\\$", "", rawTrainData$INCOME) # $ 기호 제거
rawTrainData$HOME_VAL <- gsub("\\$", "", rawTrainData$HOME_VAL) # $ 기호 제거
rawTrainData$BLUEBOOK <- gsub("\\$", "", rawTrainData$BLUEBOOK) # $ 기호 제거
rawTrainData$OLDCLAIM <- gsub("\\$", "", rawTrainData$OLDCLAIM) # $ 기호 제거

rawTrainData$INCOME <- as.numeric(rawTrainData$INCOME) # 숫자로 변환
rawTrainData$HOME_VAL <- as.numeric(rawTrainData$HOME_VAL) # 숫자로 변환
rawTrainData$BLUEBOOK <- as.numeric(rawTrainData$BLUEBOOK) # 숫자로 변환
rawTrainData$OLDCLAIM <- as.numeric(rawTrainData$OLDCLAIM) # 숫자로 변환
```

Stepwise regression analysis was performed. The purpose of stepwise regression is to identify the subset of predictors that best explain the response variable. In this case, the response variables are TARGET_AMT, KIDSDRIV, AGE, HOMEKIDS, YOJ, INCOME, HOME_VAL, TRAVTIME, BLUEBOOK, TIF, OLDCLAIM, CLM_FREQ, MVR_PTS, and CAR_AGE. The model selects the best subset of these predictors by iteratively adding or removing variables based on statistical criteria, in this case the Akaike Information Criterion (AIC).

The final model contains the variables ARGET_AMT, HOMEKIDS, INCOME, HOME_VAL, BLUEBOOK, and CAR_AGE. This subset of variables yielded the lowest AIC value among all possible subsets of variables considered by the algorithm.

To evaluate the performance of a machine learning model, it is important to split the data into training and test sets. The training set is used to train the model, while the test set is used to evaluate how well the model generalizes to new, unseen data.

```{r}
library(MASS)

set.seed(333)
trainIndex <- sample(1:nrow(rawTrainData), round(0.7*nrow(rawTrainData)), replace = FALSE)
trainData <- rawTrainData[trainIndex, ]
testData <- rawTrainData[-trainIndex, ]


trainData <- na.omit(trainData)
testData <- na.omit(testData)


model <- stepAIC(lm(TARGET_FLAG ~ TARGET_AMT + KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME + HOME_VAL + TRAVTIME + BLUEBOOK + TIF + OLDCLAIM + CLM_FREQ + MVR_PTS + CAR_AGE, data = trainData))


summary(model)

predictions <- predict(model, testData)

cor(predictions, testData$TARGET_FLAG)
```
The model's coefficient of 0.6015157 represents the correlation between the predicted values and actual values of the target variable, which is the probability of a car insurance claim being filed (TARGET_FLAG). A value of 0.60 indicates a moderate positive correlation between the predicted and actual values.

The model's coefficients for INCOME and HOME_VAL are negative, indicating that as these variables decrease, the predicted probability of a car insurance claim being filed (TARGET_FLAG) increases. This observation seems to contradict the expectation that individuals with higher incomes and more valuable homes would be safer drivers and thus less likely to file insurance claims.

If the result is counter intuitive, it might be worthwhile to conduct another experiment to verify the accuracy of the outcome.


# Model Building

# Selecting Best Model

# Conclusions and Suggestions

# References
[Binary Classification] (https://seantrott.github.io/binary_classification_R/#the_logistic_model)
[Testing for correlation with missing values] (https://campus.datacamp.com/courses/scalable-data-processing-in-r/case-study-a-preliminary-analysis-of-the-housing-data?ex=4)
# Appendix

---
title: "Insurance Quotes"
author: "Anthony Arroyo, Seung min song, Alice A. Friedman"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: 
  pdf_document:
    pandoc_args: --listings
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
(function() { set.seed(8675309) })()
invisible(lapply(list.files(here::here("R"), "\\.R$", full.names = TRUE), source))

library(magrittr)
library(here)
library(reshape2)
library(ggrepel)
library(ModelMetrics)
library(tidyverse)
library(tidyr)
library(corrplot)
library(glmnet)

rawData = LoadDataElseWeb("insurance_training_data.csv", "../data/") 
rawTestData = LoadDataElseWeb("insurance-evaluation-data.csv", "../data/")
```

# Abstract

We discuss the challenges from evaluating insurance data, assessing whether a car is involved in a crash and then how expensive the claim would be. Our goal is to create a logistic regression model, which predicts the binary value of whether or not a car has been in a crash (`TARGET_FLAG`), as well as a linear regression, which predicts the amount of the claim (`TARGET_AMT`). Our linear regression models pull some insights regarding the effects of marital status and others on claim costs. MORE TK.

# Part 1. Data Exploration 

The data has 23 features and 2 target variables that we wish to predict. As part of our predictive analytics, first we will decide if the car that belongs to a license plate has been in a crash before. Second, we will determine how much the cost of the crash was if they were.

- TARGET_FLAG: Was car in a crash? 1=YES 0=NO None
- TARGET_AMT: If car was in a crash, what was the cost of the damage?

There are some general myths related to driving that may skew our view about what we expect. How much of it is true? Provided below is a quick data dictionary of the features we will be measuring.

- AGE: Age of Driver
- BLUEBOOK: Value of Vehicle
- CAR_AGE: Vehicle Age
- CAR_TYPE: Type of Car
- CAR_USE: Vehicle Use
- CLM_FREQ: # Claims (Past 5 Years)
- EDUCATION: Max Education Level
- HOMEKIDS: # Children at Home
- HOME_VAL: Home Value
- INCOME: Income
- JOB: Job Category
- KIDSDRIV: # Driving Children
- MSTATUS: Marital Status
- MVR_PTS: Motor Vehicle Record Points
- OLDCLAIM: Total Claims (Past 5 Years)
- PARENT1: Single Parent
- RED_CAR: A Red Car
- REVOKED: License Revoked (Past 7 Years)
- SEX: Gender
- TIF: Time in Force
- TRAVTIME: Distance to Work
- URBANICITY: Home/Work Area
- YOJ: Years on Job

The top of the dataset is shown below, where we can see a smattering of isues such as untidy data and even a missing value in one of the income observations.

```{r head-class}
head(rawData, 4)
#sapply(rawData, class)
```

## Tidy Data

After carefully examining the data in the raw data frame, it was found that certain variables contained unnecessary characters such as dollar signs. To address this issue, regular expression (regex) was used to remove these unwanted characters from the variables. By doing so, the data is now cleaner and more suitable for analysis.

Additionally, there are several nominal categories that are yes and no questions. We reduced this to binary where 1=yes and 0=no using regex again to further simplify the process.

Finally, we convert variables to type factor or numeric, as appropriate.

```{r clean-funcs}
### Akin to dplyr::mutate_at but does not throw error ("silent") on missing cols and applies a function
SilentMutateAt <- function(df, dirtyCols = c("Sample", "Text"), FUN, ...) {
  ind = colnames(df) %in% dirtyCols
  df[, ind] = sapply(df[, ind], FUN = FUN, ... = ...)
  
  return(df)
}

### Rearranges parameters of gsub to be more friendly to pipe operator
GsubPipe <- function(x, pattern, replacement, ...) {
  gsub(pattern, replacement, x, ...)
}

### Replaces "yes/no" questions with an integer where yes=1 and no=0
CleanYesNo <- function(df, trace = FALSE, threshold = 0.5) {
  yesNoCols = sapply(df, function(x) { 
    unique(x) %>% 
      grepl("yes|no|^y$|^n$", ., ignore.case = T) %>% 
        { sum(.) / length(.) } %>%
          { . >= threshold }
  }) %>%
    .[.] %>%
      names(.)
  
  result = SilentMutateAt(df, yesNoCols, function(x) {
    truthy = as.integer(grepl("yes|^y$", x, ignore.case = T))
    truthy = truthy - as.integer(grepl("no|^n$", x, ignore.case = T))
    
    truthy[truthy == 0L] = NA
    truthy[truthy == -1L] = 0L
    
    truthy
  })
  
  if (trace) {
    return(list(result, yesNoCols))
  }
  
  return(result)
}

### Pre-processing data pipeline for insurance data; cleans up strings
CleanInsuranceStrings <- function(df) {
  SilentMutateAt(df, colnames(df)[sapply(df, is.character)], GsubPipe, "\\$|z_|<|,", "") %>% 
    CleanYesNo(.) %>%
      # could also use readr::parse_number here
      SilentMutateAt(c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM"), as.numeric) %>% 
        ### Coerces blank strings to NA
        SilentMutateAt(., "JOB", function(x) { x[x == ""] = "Unspecified" ; x }) %>%
          ### Converts strings to factors
          dplyr::mutate_at(., colnames(.)[sapply(., is.character)], as.factor)
}
```

```{r init}
#runs functions
cleanData <- CleanInsuranceStrings(rawData)
cleanTestData <- CleanInsuranceStrings(rawTestData)
imputeData = CleanInsuranceStrings(rawData)

#reference: https://statisticsglobe.com/convert-character-to-factor-in-r

summary(cleanData)
head(cleanData)
```

## Visulization of the data set

Our features span a multitude of different distributions.  None of them are strictly normal unless `0` values are excluded, although `AGE` comes closest. It is unclear in the data if `0` is equivalent to missing data, or whether some car owners have `0` income (many of whom are coded as students or homemakers) or a `0` home value (perhaps renters?). Several are categorical(e.g., `JOB`), ordinal (e.g., `EDUCATION`) or binomial (e.g., `RED_CAR`). 

```{r categorize}
# list of continuous vars
contVars <- c(
  "TARGET_AMT",
  "AGE",
  "BLUEBOOK",
  "CAR_AGE",
  "HOME_VAL",
  "INCOME",
  "OLDCLAIM",
  "TIF",
  "TRAVTIME",
  "YOJ"
)

# list of categorical, discrete vars
factorVars <- c(
  "CAR_TYPE",
  "CLM_FREQ",
  "EDUCATION",
  "HOMEKIDS",
  "JOB",
  "KIDSDRIV",
  "MVR_PTS"
)

# list of categorical, discrete vars
binomials <- c(
  "CAR_USE",
  "TARGET_FLAG",
  "MSTATUS",
  "PARENT1",
  "RED_CAR",
  "REVOKED",
  "SEX",
  "URBANICITY"
)

```


```{r}
### Plots the distribution of each variable
##continuous vars
nViz <- cleanData %>% tidyr::drop_na()
nViz %>% 
  select_if(is.numeric) %>%                     # Keep only numeric columns
  gather() %>%                             # Convert to key-value pairs
  ggplot(aes(value)) +                     # Plot the values
    facet_wrap(~ key, scales = "free") +   # In separate panels
    geom_density() 

discreteVarsLong <- nViz %>% dplyr::select(all_of(factorVars)) %>% gather()

#discrete vars
ggplot(data = discreteVarsLong) +
  geom_bar(mapping = aes(x = as.factor(value), fill = as.factor(value)), show.legend = FALSE) + 
  facet_wrap( ~ key, scales = "free") + 
  theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))


zero1 <- c(0,1)
releveledBi <- nViz %>% 
  dplyr::select(all_of(binomials)) %>%
  mutate(
      MALE = SEX %>% plyr::mapvalues(from = c("F", "M"), to = zero1),
      COMM_CAR = CAR_USE %>% plyr::mapvalues(from = c("Private", "Commercial"), to = zero1),
      URBAN = URBANICITY %>% plyr::mapvalues(from=c("Highly Rural/ Rural", "Highly Urban/ Urban"), to=zero1)
    ) %>%
  dplyr::select(-CAR_USE, -URBANICITY, -SEX)


#binomial vars
ggplot(data = releveledBi %>% gather(key = "key", value="value", -TARGET_FLAG)) +
  geom_bar(mapping = aes(x = key, fill=value), show.legend = TRUE, position = "fill") + 
  facet_grid(vars(TARGET_FLAG)) + 
  theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+
  ylab("Target Flag")

```

```{r dist, eval=FALSE}
### Plots the distribution of each variable
# See common-helpers.R
RemoveCols(cleanData, c("INDEX", "TARGET_FLAG", "TARGET_AMT")) %>%
  # See common-stats.R
  PlotVarDistribution(showBoxPlot = TRUE)
```

## Correlation

Several of the continuous variables are show to correlated. We see a strong correlation between `INCOME`, `HOME_VAL`, `BLUEBOOK`, and `CAR_AGE`, which intuitively makes sense as higher income people are more likely to have more expensive houses, and newer, more expensive cars.  

`INCOME` and `HOME_VAL` have the strongest correlation, with a correlation coefficient of 0.58. It is unlikely that both of these features should be used in the final model.


```{r corr}
# Numeric correlational matrix for continuous variables
cleanData %>% select(all_of(contVars)) %>% cor(use = "complete.obs") %>% round(2)

# drill down into variables identified as signifincantly correlated (corr > 0.4)
cleanData %>% select(INCOME, BLUEBOOK, HOME_VAL, CAR_AGE) %>% cor(use = "pairwise.complete.obs") %>% round(2)

# drill down into variables identified as signifincantly correlated (corr > 0.4)
## Income X HOME_VAL
plot(cleanData %>% select(INCOME, BLUEBOOK, HOME_VAL, CAR_AGE))


### Develops a correlation plot for all numeric values
rawDatanum <- cleanData %>% select(all_of(contVars)) %>% drop_na()

corrplot(cor(rawDatanum), method="square")

```
Categorical data correlations must be calculated differently. Purely categorical data correlation can be assessed using different methods. One method to compare correlation between a continuous and binomial feauture is to determine if a logistic model can be developed with a significant P value.

As shown below, many of the categorical variables are highly correlated. For example, `RED_CAR` is highly predictive of `SEX` == M. Martial status and being a single parent are (of course) mechanistically connected (all single parents are unmarried). 

```{r}
#Define a list of binomial features

binomials <- c(
  "TARGET_FLAG",
  "PARENT1",
  "MSTATUS",
  "RED_CAR",
  "REVOKED",
  "SEX"
)

# relevel M/F as binomial
binomialDF <- cleanData %>% 
  select(all_of(binomials)) %>%
  mutate(
  SEX = SEX %>% plyr::mapvalues(from = c("M", "F"), to = c(0, 1)) %>% as.numeric()
) 


library(ggcorrplot)
model.matrix(~0+., data=binomialDF) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag=FALSE, lab=TRUE, lab_size=2)
```



## Missing values
The rawData data frame contains 970 missing values, representing 948 out of 8161 observations. 

```{r missing, eval=TRUE}
### Counts values with missing data
#nrow(rawData[is.na(rawData),]) 

### total missing values
print("Total Missing Values")
colSums(is.na(rawData)) %>% sum(
)

### Counts rows with missing data
print("Total Rows with Missing Values")
sum(!complete.cases(rawData)) #observations with missing data
```

```{r missing-cols}
print("Total missing values")
colSums(is.na(cleanData)) %>% sum() #total missing values
print("Observations with missing values")
sum(!complete.cases(cleanData)) #observations with missing data
("Number of missing values by feature")
colSums(is.na(cleanData))
```
A good rule of thumb is to drop any feature with more than 80% missing values -- that is not the case with any features here. 

The remaining options are to:
1) Drop rows with missing values
2) Infer missing values

In order to decide which course to take we should first determine if there is any correlation between features with missing values, or if values are missing at random. Because several of these features are numeric, we will first need to convert the features to the appropriate data type in order to determine if there is a correlation.

The following variables in the data set have missing values: **AGE** (6), **YOJ** (454), **INCOME** (445), **HOME_VAL** (464) and **CAR_AGE**(510). **JOB** also has 526 "unspecified" values which we will recode as NA. 

First, we will drop the missing **AGE** rows, then we will test each remaining variable for correlation with the other features in the data set to determine how best to handle. 


```{r, warning=FALSE}
# Define list of columns to test
cols <- c("YOJ", "HOME_VAL", "INCOME", "CAR_AGE", "JOB")

#drop AGE nas
naTestDF <- cleanData %>% tidyr::drop_na(AGE) %>% select(-INDEX)
naTestDF["JOB"][naTestDF["JOB"]==""] <- NA #replace "Unspecified wiht NA

# Define a function to test correlation with missingness and other features
MAR_test <- function(df, cols) {
  
  # Define a function to relevel by missing values
  relevel_missing <- function(x) {
    for (i in 1:length(x)) {
      x[i] <- dplyr::if_else(is.na(x[i]), 1, 0)
      }
    return(x)
    }


  for (col in cols){
    # Create a var to hold the releveled values
    is_missing <- relevel_missing(subset(df, select=col)) %>% unlist()

    # p values we will fill in
    p_vals <- rep(NA, ncol(df)) 

    # Loop through every remaining column
    for (j in 1:(ncol(df))) {
	    s <- summary (glm(is_missing ~ df[, j], family = binomial))
	    p_vals[j] <- s$coefficients[2,4]
    }

    results <- cbind(names(df), p_vals) %>% subset(p_vals < 0.05)
    
    print("")
    print("Features with missing values")
    print(col)
    print("Features correlated with missingness")
    print(results)
  }
}

MAR_test(naTestDF, cols)

```

All the remaining features with missing values have correlation with at least one other feature, except **CAR_AGE** and **JOB**. These variables can therefore be said to plausibly be missing completely at random, and so those rows can also be dropped without any expectation of affecting the model performance.

```{r drop_missing_car_age}
trainData <- cleanData 
trainData["JOB"][cleanData["JOB"]==""] <- NA 
trainData <- trainData %>%  tidyr::drop_na(AGE, CAR_AGE, JOB) 
```
The remaining features will require imputed values.


### YOJ
YOJ missingness is strongly corelated to **EDUCATION**, which makes some sense as the more years of education someone has, the less time they have had to hold a job. One technique to deal with imputing values from an ordinal variable is to recode using some numerical values, which is straightforward to do as educaitonal attainment can be translated into years. 

```{r impute-YOJ}

#number of years associated with education levels
edYears <- c(
    12,
    16,
    18,
    20 #avg. years for phd = 8 years
)

ed_levels <- c(
   "High School",
   "Bachelors",  
   "Masters",
   "PhD" 
)

# Subset the data with missing values of YOJ, replace education levels with years
missing_YOJ <- subset(cleanData, is.na(YOJ))
  
imputeYOJ_df <-cleanData %>%
  mutate(EDUCATION = EDUCATION %>% plyr::mapvalues(from=ed_levels, to=edYears))

#predict the missing values usng a linear regression using recoded data
lm_model_YOJ <- lm(YOJ ~ EDUCATION, data = imputeYOJ_df)

predicted_YOJ <- predict(lm_model_YOJ, 
                         newdata = missing_YOJ %>%  
                           mutate(EDUCATION = EDUCATION %>% plyr::mapvalues(from=ed_levels, to=edYears)))

rounded_YOJ <- round(predicted_YOJ, digits = 0)
imputeData$YOJ[is.na(imputeData$YOJ)] <- rounded_YOJ
```

### Income

**INCOME** missingness is similarly related to **BLUEBOOK**, **MVR_PTS**, and **URBANICITY**. We can impute based on a linear regression from these three variables.
```{r impute-income}

# Subset the data with missing values of INCOME
missing_INCOME <- subset(imputeData, is.na(INCOME))
    
lm_model_INC <- lm(INCOME ~ BLUEBOOK + MVR_PTS + URBANICITY, data = imputeData)

predicted_INCOME <- predict(lm_model_INC, newdata = missing_INCOME)

rounded_INCOME <- round(predicted_INCOME, digits = 0)
imputeData[is.na(imputeData$INCOME),]$INCOME <- rounded_INCOME

```

### Home-val

Build the linear regression model with AGE, YOJ, and INCOME as predictors and HOME_VAL as the response and predict the missing values of HOME_VAL based on the values of AGE, YOJ, and INCOME. Insert the predicted values of HOME_VAL into the original dataframe

```{r impute-home}
# Subset the data with missing values of HOME_VAL
missing_HOME_VAL <- subset(imputeData, is.na(HOME_VAL))


lm_model <- lm(HOME_VAL ~ AGE + YOJ + INCOME, data = imputeData)


predicted_HOME_VAL <- predict(lm_model, newdata = missing_HOME_VAL)

#rounded_HOME_VAL <- round(predicted_HOME_VAL, digits = 0)
imputeData[is.na(imputeData$HOME_VAL),]$HOME_VAL <- predicted_HOME_VAL
imputeData$HOME_VAL <- round(imputeData$HOME_VAL, digits = 0)

head(imputeData$HOME_VAL)
```

### Car Age

To handle the missing value in CAR_AGE, we will use chained imputation since it is difficult to assess. This will also fill any other data that is still missing so we now have produced our final cleaning steps. We may next consider doing manipulations such as normalization, zero centering, and other kinds of feature engineering.

```{r impute-finish}
imputeData = imputeData %>%
  mice::mice(., method = "pmm", printFlag = FALSE) %>%
    mice::complete(.)
trainClean = imputeData
```

## Summary

```{r impute-summary}
summary(dplyr::select(imputeData, -INDEX))
```

## Box-plot

We have 971 male and 1170 female in this study. Male average income is higher than man. However, man has highest car purchase amount. The boxplots show that the medians and interquartile range of the two distributions are very close.

```{r sex-income}
SEX_df <- imputeData %>%
  dplyr::filter(!is.na(INCOME)) %>%
    dplyr::group_by(SEX) %>%
      dplyr::summarise(INCOME = mean(INCOME))

SEX_df
```

```{r plot-sex-income}
ggplot2::ggplot(imputeData) +
  ggplot2::aes(x = SEX, y = INCOME, color = SEX, fill = SEX) +
  ggplot2::geom_bar(data = SEX_df, stat = "identity", alpha = .3) +
  ggrepel::geom_text_repel(ggplot2::aes(label = HOME_VAL), color = "black", size = 2.5, segment.color = "grey") +
  ggplot2::geom_point() +
  ggplot2::guides(color = "none", fill = "none") +
  ggplot2::theme_bw() +
  ggplot2::labs(title = "Income by Sex", x = "Sex", y = "Income")
```

# Data Transformations

For our model, we will explore using power transforms on our model where features A and B can create a derived feature A*B. We have previously verified that there is very low colinearity in our data so this will be safe to use, although it will raise our computation effort.

```{r tf-prod}
nominalCols = c("KIDSDRIV", "HOMEKIDS", "CLM_FREQ")
binomialCols = c("PARENT1", "MSTATUS", "RED_CAR", "REVOKED")

### Creates features that are products of numerical columns and binds to the original dataset
# See common-helpers.R
transformedData = cbind(trainClean, CrossProdFeatures(trainClean, dropCols = c(
  "INDEX", "TARGET_FLAG", "TARGET_AMT", nominalCols, binomialCols
))) %>%
  RemoveCols(., "INDEX")

colnames(transformedData) %>%
  .[! . %in% colnames(trainClean)]
```

# Model Building

We will each create a model to explore and compare the differences between them. Additionally, we will discuss the key insights that we gain from the modeling process because it will help us draw conclusions about the dataset and potentially point us towards a preferred model.

## Seung's Models

Stepwise regression analysis was performed. The purpose of stepwise regression is to identify the subset of predictors that best explain the response variable. In this case, the response variables are TARGET_AMT, KIDSDRIV, AGE, HOMEKIDS, YOJ, INCOME, HOME_VAL, TRAVTIME, BLUEBOOK, TIF, OLDCLAIM, CLM_FREQ, MVR_PTS, and CAR_AGE. The model selects the best subset of these predictors by iteratively adding or removing variables based on statistical criteria, in this case the Akaike Information Criterion (AIC).

The final model contains the variables TARGET_AMT, KIDSDRIV, AGE, HOMEKIDS, YOJ, HOME_VAL, TRAVTIME, BLUEBOOK, TIF, OLDCLAIM, CLM_FREQ ,MVR_PTS, CAR_AGE. This subset of variables yielded the lowest AIC value among all possible subsets of variables considered by the algorithm.

To evaluate the performance of a machine learning model, it is important to split the data into training and test sets. The training set is used to train the model, while the test set is used to evaluate how well the model generalizes to new, unseen data.

```{r seung-model}
library(MASS)

set.seed(333)
trainIndex <- sample(1:nrow(trainClean), round(0.7*nrow(trainClean)), replace = FALSE)
trainData <- trainClean[trainIndex, ]
testData <- trainClean[-trainIndex, ]


trainData <- na.omit(trainData)
testData <- na.omit(testData)


model <- stepAIC(lm(TARGET_FLAG ~ TARGET_AMT + KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME + HOME_VAL + TRAVTIME + BLUEBOOK + TIF + OLDCLAIM + CLM_FREQ + MVR_PTS + CAR_AGE, data = trainData))


summary(model)

predictions <- predict(model, testData)

cor(predictions, testData$TARGET_FLAG)
```

The model's coefficient of 0.5860083 represents the correlation between the predicted values and actual values of the target variable, which is the probability of a car insurance claim being filed (TARGET_FLAG). A value of 0.60 indicates a moderate positive correlation between the predicted and actual values.

The model's coefficients for INCOME and HOME_VAL are negative, indicating that as these variables decrease, the predicted probability of a car insurance claim being filed (TARGET_FLAG) increases. This observation seems to contradict the expectation that individuals with higher incomes and more valuable homes would be safer drivers and thus less likely to file insurance claims.

If the result is counter intuitive, it might be worthwhile to conduct another experiment to verify the accuracy of the outcome.

```{r}
confint(model)
```

Multiple Regression Model
```{r}
fit=lm(TARGET_FLAG ~ TARGET_AMT + KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME + HOME_VAL + TRAVTIME + BLUEBOOK + TIF + OLDCLAIM + CLM_FREQ + MVR_PTS + CAR_AGE, data = trainData)
par(mfrow=c(2,2))
plot(fit)
par(mfrow=c(1,1))
```


## Anthony's First Models

Of the models we build, we will build our first two models after an exhaustive outlier cleaning. It seems that both models favor managers for a lower insurance rate, interestingly enough. Additionally, the type of car has a significant impact on the amount. Also, both models agree that married couples are charged significantly lower rates, whereas this is even worse in the event that an unmarried person is also a single parent. Finally, both models agree that it is extremely significant whether the drivers are in urban or rural areas, such that urban drivers get charged around $1500 more than their rural counterparts.

```{r a-model1, eval=FALSE, warning=FALSE}
.DataTransformsPipeline <- function(df, removeTargetCol = c("TARGET_FLAG", "TARGET_AMT")[1], nonLinearCols) {
  RemoveCols(df, removeTargetCol) %>%
    CoerceOutliersToNa(4) %>%
      CoerceOutliersToNa(4) %>%
        CoerceOutliersToNa(4) %>%
          CoerceOutliersToNa(3) %>%
            { dplyr::coalesce(
              RemoveCols(transformedData, nonLinearCols, invert = T),
              .
            )} %>%
              dplyr::select(colnames(df)) %>%
                mice::mice(., method = "lasso.norm", printFlag = FALSE) %>%
                  mice::complete(.)
}
```

```{r a-model2, eval=FALSE, warning=FALSE}
lmMockup = .DataTransformsPipeline(
  transformedData,
  "TARGET_FLAG",
  c("TARGET_FLAG", "TARGET_AMT", "OLDCLAIM", nominalCols, binomialCols)
) %>%
  RemoveCols("TARGET_FLAG") %>%
    { do.call("lm", list(formula = TARGET_AMT ~ ., data = .)) }

lmModel = MASS::stepAIC(lmMockup, direction = "both", trace = FALSE)
funnyModel = lmModel

saveRDS(funnyModel, paste0(here::here("data/models"), "/funnyModel.rds"))
```

```{r a-model3, eval=FALSE, warning=FALSE}
lmMockupAlt = .DataTransformsPipeline(
  cleanData,
  "TARGET_FLAG",
  c("TARGET_FLAG", "TARGET_AMT", "OLDCLAIM", nominalCols, binomialCols)
) %>%
  RemoveCols("TARGET_FLAG") %>%
    { do.call("lm", list(formula = TARGET_AMT ~ ., data = .)) }

lmModelAlt = MASS::stepAIC(lmMockupAlt, direction = "both", trace = FALSE)
smallerModel = lmModelAlt

saveRDS(smallerModel, paste0(here::here("data/models"), "/smallerModel.rds"))
```

```{r a-load-models}
funnyModel = readRDS(paste0(here::here("data/models"), "/funnyModel.rds"))
smallerModel = readRDS(paste0(here::here("data/models"), "/smallerModel.rds"))

### Subsets the model coefficients to those of pvalue below a value
summary.lm(funnyModel)$coefficients %>%
  .[.[,4] < .05, ]
```


## Additional Transforms

Since we created our initial models, we decided that there are three additional transforms we want to add. We believe that having a claim is predictive of making a claim again, so we will have a boolean column for any number of claims. Additionally, we decided that the only two relevant job types are doctors and managers, based on the models. Therefore, we will also make two boolean columns for if they are a manager or doctor.

```{r tf-ins}
### Creates a factor column for Manager/Doctor jobs and no claim history, specific to insurance dataset
InsuranceTransforms <- function(df) {
  df %>%
    dplyr::mutate(
      IS_MANAGER = as.factor(as.integer(JOB %in% "Manager")),
      IS_DOCTOR = as.factor(as.integer(JOB %in% "Doctor")),
      NO_CLM_HIST = as.factor(as.integer(CLM_FREQ < 1))
    ) %>%
      dplyr::select(-JOB, -CLM_FREQ)
}
```

## Further Notes

Although new models were attempted with the new transforms and additional considerations, none of them successfully beat the existing model by a statistically significant margin. We compared each of the models using ANOVA and evaluated the LM models with MAE and RMSE, whereas the logistical models were compared using F1 and Accuracy metrics. Overall, we kept the previously accepted models due to the lack of advancement.

```{r meh-model, eval=FALSE}
### Shuffles a vector to allow random picking simulations (not used)
Shuffle <- function(x) {
  x[sample(seq_along(x))]
}

### A plug and play for the insurance dataset
MoreFactors <- function(df, cols = c(nominalCols, binomialCols)) {
  dplyr::mutate_at(df, cols, as.factor)
}

### Adds three factors columns
threeAddedData = cleanData %>%
  RemoveCols("INDEX") %>%
    InsuranceTransforms(.) %>%
      mice::mice(., method = "lasso.norm", printFlag = FALSE) %>%
        mice::complete(.)

### Converts many more columns into factors that weren't previously
factorsData = cleanData %>%
  RemoveCols("INDEX") %>%
    InsuranceTransforms(.) %>%
      MoreFactors(., c(nominalCols, binomialCols) %>% .[! . %in% "CLM_FREQ"]) %>%
        mice::mice(., method = "lasso.norm", printFlag = FALSE) %>%
          mice::complete(.)

ModelInsurance <- function(df, seed = 8675309) {
  set.seed(seed)

  tts = rsample::initial_split(df, strata = TARGET_FLAG)
  zdfTrain = rsample::training(tts)
  zdfTest = rsample::testing(tts)

  lmResults = zdfTrain %>%
    RemoveCols("TARGET_FLAG") %>%
      { do.call("lm", list(formula = TARGET_AMT ~ ., data = .)) }

  logResults = zdfTrain %>%
    RemoveCols("TARGET_AMT") %>%
      { do.call("glm", list(formula = TARGET_FLAG ~ ., data = ., family = "binomial"))}

  lmOptimized = MASS::stepAIC(lmResults, direction = "both", trace = FALSE)
  logOptimized = MASS::stepAIC(logResults, direction = "both", trace = FALSE)

  vr = zdfTest$TARGET_AMT
  v1 = predict.lm(lmResults, zdfTest)
  v2 = predict.lm(lmOptimized, zdfTest)

  lmTbl = data.frame(
    mae = c(ModelMetrics::mae(vr, v1), ModelMetrics::mae(vr, v2)),
    rmse = c(ModelMetrics::rmse(vr, v1), ModelMetrics::rmse(vr, v2)),
    rmsle = c(ModelMetrics::rmsle(vr, v1), ModelMetrics::rmsle(vr, v2)),
    cor = c(cor(vr, v1), cor(vr, v2))
  )
  logTbl = suppressWarnings(CompareModelStats(zdfTest, "TARGET_FLAG", logResults, logOptimized))

  list(list(lmResults, lmOptimized), list(logResults, logOptimized), list(lmTbl, logTbl))
}

clm = ModelInsurance(trainClean)
lmModel = clm[[1]][[1]]
logModel = clm[[2]][[1]]
```

```{r more-tests, eval=FALSE}
# Not the best integration...
zlm = ModelInsurance(threeAddedData)
tlm = ModelInsurance(transformedData)
ilm = ModelInsurance(imputeData)
flm = ModelInsurance(factorsData)

logTables = lapply(list(clm, zlm, tlm, ilm, flm), function(x) { x[[3]][[2]] }) %>%
  do.call(rbind, .)

lmTables = lapply(list(clm, zlm, tlm, ilm, flm), function(x) { x[[3]][[1]] }) %>%
  do.call(rbind, .)
```

```{r test-excess, eval=FALSE}
### No significant improvement
olm = cleanData %>%
  RemoveCols(., names(sapply(cleanData, is.factor) %>% .[.])) %>%
    RemoveCols("KIDSDRIV") %>%
    CoerceOutliersToNa(4) %>%
      CoerceOutliersToNa(4) %>%
      CoerceOutliersToNa(4) %>%
      CoerceOutliersToNa(3) %>%
      mice::mice(., method = "pmm", printFlag = FALSE) %>%
      mice::complete(.) %>%
      cbind(., cleanData[, sapply(cleanData, is.factor)], cleanData["KIDSDRIV"]) %>%
      InsuranceTransforms(.) %>%
      dplyr::select((colnames(cleanData) %>% .[! . %in% c("JOB", "CLM_FREQ")]), dplyr::everything()) %>%
      RemoveCols(c("INDEX", "JOB")) %>%
      mice::mice(., method = "lasso.norm", printFlag = FALSE) %>%
      mice::complete(.) %>%
      ModelInsurance(.)
```

```{r lm-assist, eval=FALSE}
# An experiment to see if RMSE is reduced if we exclude non-claims cases
# Note, this only worsened the results: MAE += 600; RMSE += 1800

### Filters claims cases only, except 1 non-claim case
Crash <- function(df) {
  rbind(df[df$TARGET_FLAG == 1, ], df[(df$TARGET_FLAG == 0)[1], ])
}

clmCrash = ModelInsurance(Crash(trainClean))
zlmCrash = ModelInsurance(Crash(threeAddedData))
tlmCrash = ModelInsurance(Crash(transformedData))
ilmCrash = ModelInsurance(Crash(imputeData))
flmCrash = ModelInsurance(Crash(factorsData))

logCrashTables = lapply(list(clmCrash, zlmCrash, tlmCrash, ilmCrash, flmCrash), function(x) { x[[3]][[2]] }) %>%
  do.call(rbind, .)

lmCrashTables = lapply(list(clmCrash, zlmCrash, tlmCrash, ilmCrash, flmCrash), function(x) { x[[3]][[1]] }) %>%
  do.call(rbind, .)

logCrashTables
lmCrashTables
```

# Selecting Best Linear Regression Model

```{r}
testClean = CleanInsuranceStrings(rawTestData) %>%
  mice::mice(., method = "pmm", printFlag = FALSE) %>%
    mice::complete(.)

results = data.frame(
  INDEX = testClean$INDEX,
  TARGET_FLAG = ifelse(predict(logModel, testClean) <= 0.5, 0L, 1L),
  TARGET_AMT = predict(lmModel, testClean)
) %>%
  dplyr::mutate(TARGET_AMT = round(TARGET_AMT * TARGET_FLAG, 2)) %>%
    write.csv(paste0(here::here("docs"), "/insurance-quotes-", substring(Sys.time(), 1, 10), ".csv"), row.names = FALSE)
```

# Selecting Best Logistic Regression Model
Need to add a section here about the best logistic model.

# Conclusions and Suggestions

# References

https://www.statology.org/correlation-between-categorical-variables/

# Appendix

```{r, echo=TRUE, class.output="foldable"}
CleanInsuranceStrings
```

```{r, echo=TRUE, class.output="foldable"}
InsuranceTransforms
```

```{r, echo=TRUE, class.output="foldable"}
ModelInsurance
```


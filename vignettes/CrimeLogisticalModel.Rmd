---
title: "Crime Logistical Model"
author: "Anthony A, IvanTikhonov, Seung min song"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: 
  pdf_document:
    pandoc_args: --listings
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r libraries, include=FALSE, warning=FALSE}
library(dplyr)
library(reshape2)
library(ggplot2)
library(MASS)
library(dplyr)
library(dbplyr)
library(caret)
library(lattice)
library(purrr)
library(kableExtra)
```

# Abstract

We will evaluate data related to neighborhood crime and determine if a neighborhood is above the median crime rate.
To do this, we will create a logistical model that rates either true or false.
This information is necessary to determine if a neighborhood is going through difficult times or is over-policed by authorities.
As such, our approach attempts to find a non-biased answer based on what is known.

# Datasets

Datasets are provided by CUNY School of Professional Studies for academic purposes.
It is reflective of public data gathered online.

# Data Exploration

The data has 12 features and 1 target variable.
Since our model will be logistical regression, a positive result is 1 and vice versa.

- zn: proportion of residential land zoned for large lots (over 25000 square feet) 
- indus: proportion of non-retail business acres per suburb 
- chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0) 
- nox: nitrogen oxides concentration (parts per 10 million) 
- rm: average number of rooms per dwelling 
- age: proportion of owner-occupied units built prior to 1940 
- dis: weighted mean of distances to five Boston employment centers 
- rad: index of accessibility to radial highways 
- tax: full-value property-tax rate per $10,000 
- ptratio: pupil-teacher ratio by town 
- lstat: lower status of the population (percent) 
- medv: median value of owner-occupied homes in $1000s 
- target: whether the crime rate is above the median crime rate (1) or not (0)

## Data acquisition

```{r}
all_train <- read.csv("https://raw.githubusercontent.com/IvanGrozny88/DATA-621-HW3/main/crime-training-data_modified.csv")
eval <- read.csv("https://raw.githubusercontent.com/IvanGrozny88/DATA-621-HW3/main/crime-evaluation-data_modified.csv")
```

Revisit the data structure to gain a better understanding.
```{r}
glimpse(all_train)
```

## Summary Statistics

Upon analyzing the target variable, we observe that 237 out of the total observations have a crime rate below the median, whereas 229 have a crime rate above the median. Consequently, our training data set comprises an almost equal number of neighborhoods categorized as at-risk and not-at-risk.
```{r}
summary(all_train)
```
## Correlation

The variable **nox (0.72610622)**, **rm (-0.15255334)**, **age (0.63010625)**, **dis (-0.61867312)**, **rad (0.62810492)**, **tax (0.61111331)**, **lstat (0.46912702)** is correlated to the target variable, as seen in the correlation matrix. 

```{r}
cor_matrix <- cor(all_train)
print(cor_matrix)
```

```{r}
cor_with_target <- cor_matrix["target", -ncol(cor_matrix)]
print(cor_with_target)
```

## Data structure

There are 466 observations and 13 variables in the training dataset.

Additionally, there are no missing values so imputation is not necessary on this dataset.
Our minimum sample size at n% probability would be 10*13/n so our sample size is adequate (466 > 260) where 50% of samples are expected to be over the median by definition.
Shown below is a sample of its head.

```{r}
str(all_train)
```

To enhance the readability of the data, we have made some adjustments. Specifically, we have converted the binary values of the "chas" variable to categorical values of "Y" and "N", indicating whether or not the neighborhood borders the Charles River. Similarly, we have converted the binary values of the "target" variable to categorical values of "Above" and "Below", based on whether the crime rate is above or below the median. As both "chas" and "target" are categorical variables, we have changed their class from integer to factor.

```{r}
all_train$chas <- as.factor(all_train$chas)
all_train$target <- as.factor(all_train$target)
```

## Missing values

The dataset is complete with no missing values.

```{r}
nrow(all_train[is.na(all_train),])
```

## Visulization of the data set

Examine the shapes and distributions of the numerical variables by visualizing their density plots.

```{r}
datasub = reshape2::melt(all_train, id=c("chas", "target"))
ggplot(datasub, aes(x = value)) + 
    geom_density(fill = "skyblue") + 
    facet_wrap(~variable, scales = 'free') 
```

For categorial variable chas, we can look at a confusion matrix table to make sure that we have enough observations for all levels:

```{r}
xtabs(~ target + chas, data=all_train)
```
```{r}
# Create a confusion matrix
conf_mat <- matrix(c(225, 12, 208, 21), nrow = 2, byrow = TRUE)
```

```{r}
# Calculate precision, recall, and F1 score
precision <- conf_mat[2, 2] / sum(conf_mat[, 2])
recall <- conf_mat[2, 2] / sum(conf_mat[2, ])
f1_score <- 2 * precision * recall / (precision + recall)
```

```{r}
# Print the results
cat(sprintf("Precision: %.2fn", precision))
cat(sprintf("Recall: %.2fn", recall))
cat(sprintf("F1 score: %.2fn", f1_score))
```
The F1 score, which ranges from 0 to 1 and indicates the model's precision and recall, suggests that the model is not performing well. A score of 1 would indicate perfect precision and recall, while a score of 0 would indicate poor performance. With a score of 0.16, the model's performance is considered poor.

Next, examine the boxplots of the numerical variables with respect to the target variable.

```{r}
ggplot(datasub, aes(x = target, y = value, fill = target)) + 
    geom_boxplot() + 
    facet_wrap(~variable, scales = 'free') +
    scale_fill_manual(values = c('skyblue', '#FFC0CB'))
```


# Data Preparation

## Outlier Imputation

From the boxplots above, we can see there are many outliers so we are going to fix them by replacing with medians.

```{r}
train_clean_pre <- all_train 
set.seed(121)
split <- caret::createDataPartition(train_clean_pre$target, p=0.85, list=FALSE)
train_clean <- train_clean_pre[split, ]
validation <- train_clean_pre[ -split, ]
```

Letâ€™s look at the boxplots again after the outliers being imputed with median.

```{r}
ggplot(reshape2::melt(train_clean, id=c("chas", "target")), aes(x = target, y = value)) + 
    geom_boxplot() + 
    facet_wrap(~variable, scales = 'free') 
```

# Logistical Model Building

We first create a full model by including all the variables:

```{r}
fullMod <- glm(target ~ ., data = train_clean, family = 'binomial')
summary(fullMod)
```

## Model 1 - P-values Selection

From the full model, we select the variables that have small p-values:

target ~ nox + age + rad + tax + ptratio + lstat

```{r}
logMod1 <- glm(target ~ nox + age + rad + tax + ptratio + lstat, 
               data = train_clean, 
               family = 'binomial')
summary(logMod1)
```

## Model 2 - Backward Selection

```{r}
logMod2 <- fullMod %>% stepAIC(direction = "backward", trace = FALSE)
summary(logMod2)
```

## Model 3 - Forward Selection

```{r}
# Create an empty model with no variables
emptyMod <- glm(target ~ 1, data = train_clean, family = 'binomial')
logMod3 <- emptyMod %>% 
  stepAIC(direction = "forward",
          scope = ~ zn + indus + chas + nox + rm + age + dis 
                    + rad + tax + ptratio + lstat + medv, 
          trace = FALSE)
summary(logMod3)
```

# Model Selection

```{r}
preds1 =predict(logMod1, newdata = validation)
preds2 =predict(logMod2, newdata = validation)
preds3 =predict(logMod3, newdata = validation)
preds1[preds1 >= 0.5] <- 1
preds1[preds1 < 0.5] <- 0
preds1 = as.factor(preds1)
preds2[preds2 >= 0.5] <- 1
preds2[preds2 < 0.5] <- 0
preds2 = as.factor(preds2)
preds3[preds3 >= 0.5] <- 1
preds3[preds3 < 0.5] <- 0
preds3 = as.factor(preds3)
```


```{r}
m1cM <- confusionMatrix(preds1, validation$target, mode = "everything")
m2cM <- confusionMatrix(preds2, validation$target, mode = "everything")
m3cM <- confusionMatrix(preds3, validation$target, mode = "everything")
```

```{r}
formula(logMod1) # Model 1 formula
```

```{r}
fourfoldplot(m1cM$table, color = c("#B22222", "#2E8B57"), main="Model 1")
```

```{r}
formula(logMod2) # Model 2 formula
```

```{r}
fourfoldplot(m2cM$table, color = c("#B22222", "#2E8B57"), main="Model 2")
```

```{r}
formula(logMod3) # Model 3 formula
```

```{r}
fourfoldplot(m3cM$table, color = c("#B22222", "#2E8B57"), main="Model 3")
```

accuracy and lower error rate

```{r}
temp <- data.frame(m1cM$overall, 
                   m2cM$overall, 
                   m3cM$overall) %>%
  t() %>%
  data.frame() %>%
  dplyr::select(Accuracy) %>%
  mutate(Classification_Error_Rate = 1-Accuracy)
Summ_Stat <-data.frame(m1cM$byClass, 
                   m2cM$byClass, 
                   m3cM$byClass) %>%
  t() %>%
  data.frame() %>%
  cbind(temp) %>%
  mutate(Model = c("Model 1", "Model 2", "Model 3")) %>%
  dplyr::select(Model, Accuracy, Classification_Error_Rate, Precision, Sensitivity, Specificity, F1) %>%
  mutate_if(is.numeric, round,3)# %>%
  #kable('html', escape = F) %>%
  #kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),full_width = F)
Summ_Stat
```

Model 2,3 has better AUC

```{r}
getROC <- function(model) {
    name <- deparse(substitute(model))
    pred.prob1 <- predict(model, newdata = validation)
    p1 <- data.frame(pred = validation$target, prob = pred.prob1)
    p1 <- p1[order(p1$prob),]
    rocobj <- pROC::roc(p1$pred, p1$prob, levels = c(0, 1), direction = "<")
    plot(rocobj, asp=NA, legacy.axes = TRUE, print.auc=TRUE,
         xlab="Specificity", main = name)
}
par(mfrow=c(2,2))
getROC(logMod1)
getROC(logMod2)
getROC(logMod3)
```

Based on comparison result, we eventually choose Model 2/3 (same model)

Make Prediction

```{r}
eval$chas <- as.factor(eval$chas)
prediction = predict(logMod2, newdata = eval)
prediction[prediction >= 0.5] <- 1
prediction[prediction < 0.5] <- 0
prediction = as.factor(prediction)
prediction
```



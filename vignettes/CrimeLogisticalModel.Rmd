---
title: "Crime Logistical Model"
author: "Anthony A, IvanTikhonov, Seung min song"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: 
  pdf_document:
    pandoc_args: --listings
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r libraries, include=FALSE, warning=FALSE, class.output="scroll-less"}
library(dplyr)
library(reshape2)
library(ggplot2)
library(MASS)
library(dplyr)
library(dbplyr)
library(caret)
library(lattice)
library(purrr)
library(kableExtra)
invisible(lapply(list.files(here::here("R"), "\\.R$", full.names = TRUE), source))
```

# Abstract

We will evaluate data related to neighborhood crime and determine if a neighborhood is above the median crime rate.
To do this, we will create a logistical model that rates either true or false.
This information is necessary to determine if a neighborhood is going through difficult times or is over-policed by authorities.
As such, our approach attempts to find a non-biased answer based on what is known.

# Data Exploration

The data has 12 features and 1 target variable.
Since our model will be logistical regression, a positive result is 1 and vice versa.

- zn: proportion of residential land zoned for large lots (over 25000 square feet) 
- indus: proportion of non-retail business acres per suburb 
- chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0) 
- nox: nitrogen oxides concentration (parts per 10 million) 
- rm: average number of rooms per dwelling 
- age: proportion of owner-occupied units built prior to 1940 
- dis: weighted mean of distances to five Boston employment centers 
- rad: index of accessibility to radial highways 
- tax: full-value property-tax rate per $10,000 
- ptratio: pupil-teacher ratio by town 
- lstat: lower status of the population (percent) 
- medv: median value of owner-occupied homes in $1000s 
- target: whether the crime rate is above the median crime rate (1) or not (0)

## Data acquisition

```{r}
all_train <- read.csv("https://raw.githubusercontent.com/IvanGrozny88/DATA-621-HW3/main/crime-training-data_modified.csv")
all_test <- read.csv("https://raw.githubusercontent.com/IvanGrozny88/DATA-621-HW3/main/crime-evaluation-data_modified.csv")
```

Revisit the data structure to gain a better understanding.
```{r}
glimpse(all_train)
```

## Summary Statistics

Upon analyzing the target variable, we observe that 237 out of the total observations have a crime rate below the median, whereas 229 have a crime rate above the median. Consequently, our training data set comprises an almost equal number of neighborhoods categorized as at-risk and not-at-risk.
```{r}
summary(all_train)
```

## Correlation

```{r, include=FALSE}
(
cor_matrix <- cor(all_train)
)
```

According to our correlation matrix, the nox, age, rad, tax, and indus variables show moderate positive correlation (>0.6).
Additionally, the dis variable shows moderate negative correlation (< -0.6)

```{r}
(
cor_with_target <- cor_matrix["target", colnames(cor_matrix) != "target"]
)
```

## Data structure

There are 466 observations and 13 variables in the training dataset.

Additionally, there are no missing values so imputation is not necessary on this dataset.
Our minimum sample size at n% probability would be 10*13/n so our sample size is adequate (466 > 260) where 50% of samples are expected to be over the median by definition.
Shown below is a sample of its head.

```{r}
str(all_train)
```

To enhance the readability of the data, we have made some adjustments. Specifically, we have converted the binary values of the "chas" variable to categorical values of "Y" and "N", indicating whether or not the neighborhood borders the Charles River. Similarly, we have converted the binary values of the "target" variable to categorical values of "Above" and "Below", based on whether the crime rate is above or below the median. As both "chas" and "target" are categorical variables, we have changed their class from integer to factor.

```{r}
all_train$chas <- as.factor(all_train$chas)
all_train$target <- as.factor(all_train$target)
```

## Missing values

The dataset is complete with no missing values.

```{r}
nrow(all_train[is.na(all_train),])
```

## Visulization of the data set

Examine the shapes and distributions of the numerical variables by visualizing their density plots.

```{r}
datasub = reshape2::melt(all_train, id=c("chas", "target"))
ggplot(datasub, aes(x = value)) + 
    geom_density(fill = "skyblue") + 
    facet_wrap(~variable, scales = 'free') 
```

Next, examine the boxplots of the numerical variables with respect to the target variable.

```{r}
ggplot(datasub, aes(x = target, y = value, fill = target)) + 
    geom_boxplot() + 
    facet_wrap(~variable, scales = 'free') +
    scale_fill_manual(values = c('skyblue', '#FFC0CB'))
```


# Data Preparation

## Outlier Imputation

From the boxplots above, we can see there are many outliers so we are going to fix them by replacing with medians.

```{r}
train_clean_pre <- all_train 
set.seed(121)
split <- caret::createDataPartition(train_clean_pre$target, p=0.85, list=FALSE)
train_clean <- train_clean_pre[split, ]
validation <- train_clean_pre[ -split, ]
```

Letâ€™s look at the boxplots again after the outliers being imputed with median.

```{r}
ggplot(reshape2::melt(train_clean, id=c("chas", "target")), aes(x = target, y = value)) + 
    geom_boxplot() + 
    facet_wrap(~variable, scales = 'free') 
```

# Logistical Model Building

To begin, we will create a null model that does not make any prediction.
This will help us verify our first model works better than random guessing.
If we set our alpha value to 0.05 as common, the p value of almost 1 greatly exceeds it.
We hope to at least have a model with a p value below 0.05 to deem it statistically valuable.

```{r nullModel}
lmNull = rep(0, nrow(all_train["target"])) %>%
  { .[1] = 1 ; . } %>%
    as.factor(.) %>%
      cbind(., all_train["target"]) %>%
        magrittr::set_colnames(., c("X", "y")) %>%
          stats::glm(y ~ X, data = ., family = 'binomial')

summary.lm(lmNull)
```

## Model 1 - Full Model

We will start with a top-down approach and begin by including all of the variables.
According to this, our most statistically consistent variables are those with extremely low p-values.
Notably, the variables nox and rad both have very high significance bellow 0.001 so we will definitely use these in the future.

# TODO: Insert explanation about why this beats the null model

```{r}
modelOne <- stats::glm(target ~ ., data = train_clean, family = 'binomial')
summary(modelOne)
```

## Model 2 - P-values Selection

Before we begin stepwise addition and subtraction, let's also run a model with only variables with significance below 0.05 value.
This will give us a good baseline for later analysis.

```{r}
modelTwo <- stats::glm(target ~ nox + age + rad + tax + ptratio + lstat, 
               data = train_clean, 
               family = 'binomial')
summary(modelTwo)
```

## Model 3 - Backward Selection

Next, let's use the AIC scoring mechanism to do backwards stepwise addition.
This will give us more variables to compare as long as the threshold is below a significant point.

# TODO: Elaborate.

```{r}
modelThree <- MASS::stepAIC(modelTwo, direction = "backward", trace = FALSE)
summary(modelThree)
```

## Model 4 & 5 - Forward Selection

Finally, let's use that same AIC scoring mechanism to do forwards stepwise subtraction.
This will reduce some of the extra variables we added while still constraining ourselves to the most optimal fit.
Notably, when we run forward selection on our second model, it reaches a local maximum and stops.
However, if we run forwards AIC on model one, it reaches a different local maximum with a lower AIC than this one.

```{r}
modelFour <- MASS::stepAIC(modelTwo, direction = "forward", trace = FALSE)
summary(modelFour)
```

This fifth model, nicknamed 1-F, when compared to the previous model, 2-F, goes from AIC 210 to 201.
Therefore, it should hypothetically be better.
However, we will use hypothesis testing to accept or deny this model since we only want to change our model when it is statistically significant.

```{r}
modelFive <- MASS::stepAIC(modelOne, direction = "forward", trace = FALSE)
summary(modelFive)
```

# Model Selection

Our first model for consideration is the full model.

```{r, include=FALSE}
MakePredictions <- function(model, testData, excludeCol = "target", threshold = 0.5) {
  probability = stats::predict(model, testData[, ! colnames(testData) %in% excludeCol])
  class = probability %>%
    { .[. > threshold] = 1 ; . } %>%
      { .[. <= threshold] = 0 ; . } %>%
        as.factor(.)

  data.frame(class, probability)
}

VerifyKfold <- function(model, testData, excludeCol = "target", threshold = 0.5) {
  MakePredictions(model, testData, excludeCol, threshold) %>%
    { caret::confusionMatrix(.$class, testData[, colnames(testData) %in% excludeCol], mode = "everything") }
}

ValidationPipeline <- function(model, testData, excludeCol, plotname = "", threshold = 0.5) {
  p = MakePredictions(model, testData, excludeCol)
  d = VerifyKfold(model, testData, excludeCol)
  g = graphics::fourfoldplot(d$table, color = c("#B22222", "#2E8B57"), main = plotname)
  
  print(formula(model))
  print(d)
  print(g)
  
  invisible(list(p, d, g))
}

ValidationPipeline(modelOne, validation, "target", "Model 1")
```

```{r}
formula(modelOne)
predictions = MakePredictions(modelOne, validation, "target")
cv1 = VerifyKfold(modelOne, validation, "target")
cv1
graphics::fourfoldplot(cv1$table, color = c("#B22222", "#2E8B57"), main="Model 1")
```

Our second model for consideration is the manually selected p values model.

```{r}
formula(modelTwo)
cv2 = VerifyKfold(modelTwo, validation, "target")
cv2
graphics::fourfoldplot(cv2$table, color = c("#B22222", "#2E8B57"), main="Model 2")
```

Our third model is the AIC backwards selection model.

```{r}
formula(modelThree)
cv3 = VerifyKfold(modelThree, validation, "target")
cv3
graphics::fourfoldplot(cv3$table, color = c("#B22222", "#2E8B57"), main="Model 2")
```

Our fourth model is the AIC forwards selection model from model two, nicknamed 2-F in comparison to the next model.

```{r}
formula(modelFour)
cv4 = VerifyKfold(modelThree, validation, "target")
cv4
graphics::fourfoldplot(cv4$table, color = c("#B22222", "#2E8B57"), main="Model 2", space = 0.4)
```

Our fifth model is the AIC forwards selection model from model one, nicknamed 1-F in comparison to the previous model.

```{r}
formula(modelThree)
cv5 = VerifyKfold(modelThree, validation, "target")
cv5
graphics::fourfoldplot(cv5$table, color = c("#B22222", "#2E8B57"), main="Model 2")
```


accuracy and lower error rate

```{r}
temp <- data.frame(cv1$overall,
                   cv2$overall,
                   cv3$overall,
                   cv4$overall,
                   cv5$overall) %>%
  t() %>%
  data.frame() %>%
  dplyr::select(Accuracy) %>%
  mutate(Classification_Error_Rate = 1 - Accuracy)
Summ_Stat <- data.frame(cv1$byClass,
                        cv2$byClass,
                        cv3$byClass,
                        cv4$byClass,
                        cv5$byClass) %>%
  t() %>%
  data.frame() %>%
  cbind(temp) %>%
  mutate(Model = c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5")) %>%
  dplyr::select(Model,
                Accuracy,
                Classification_Error_Rate,
                Precision,
                Sensitivity,
                Specificity,
                F1) %>%
  mutate_if(is.numeric, round, 3)# %>%
#kable('html', escape = F) %>%
#kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),full_width = F)
Summ_Stat
```

Model 2,3 has better AUC

```{r}
getROC <- function(model) {
    name <- deparse(substitute(model))
    pred.prob1 <- predict(model, newdata = validation)
    p1 <- data.frame(pred = validation$target, prob = pred.prob1)
    p1 <- p1[order(p1$prob),]
    rocobj <- pROC::roc(p1$pred, p1$prob, levels = c(0, 1), direction = "<")
    plot(rocobj, asp=NA, legacy.axes = TRUE, print.auc=TRUE,
         xlab="Specificity", main = name)
}
par(mfrow=c(2,2))
getROC(modelTwo)
getROC(modelThree)
getROC(modelFour)
```

Based on comparison result, we eventually choose Model 2/3 (same model)

Make Prediction

```{r}
all_test$chas <- as.factor(all_test$chas)
prediction = predict(modelThree, newdata = all_test)
prediction[prediction >= 0.5] <- 1
prediction[prediction < 0.5] <- 0
prediction = as.factor(prediction)
prediction
```

# Conclusions and Final Thoughts

None at all.

# References

Datasets are provided by CUNY School of Professional Studies for academic purposes.
It is reflective of public data gathered online.

# Appendix

Shown here is a copy of all relevant R code.

```{r, class.output="foldable"}
MakePredictions
```

```{r, class.output="foldable"}
VerifyKfold
```

```{r, class.output="foldable"}
getROC
```

---
title: "Crime Logistical Model"
author: "Anthony A, IvanTikhonov, Seung min song, Alice A. Friedman"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: 
  pdf_document:
    pandoc_args: --listings
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r libraries, include=FALSE, warning=FALSE, class.output="scroll-less"}
library(dplyr)
library(reshape2)
library(ggplot2)
library(MASS)
library(dplyr)
library(dbplyr)
library(caret)
library(lattice)
library(purrr)
library(kableExtra)
invisible(lapply(list.files(here::here("R"), "\\.R$", full.names = TRUE), source))
```

# Abstract

We will evaluate data related to neighborhood crime to create a model to predict if a neighborhood is "high crime" which we are defining as above the median crime rate. To do this, we will create a logistical model that rates either true or false.

# Data Exploration

The data has 12 features and the training data set additionally has 1 target variable, which marks neighborhoods as "high crime" (target==1) or not high crime (target==0).

- zn: proportion of residential land zoned for large lots (over 25000 square feet) 
- indus: proportion of non-retail business acres per suburb 
- chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0) 
- nox: nitrogen oxides concentration (parts per 10 million) 
- rm: average number of rooms per dwelling 
- age: proportion of owner-occupied units built prior to 1940 
- dis: weighted mean of distances to five Boston employment centers 
- rad: index of accessibility to radial highways 
- tax: full-value property-tax rate per $10,000 
- ptratio: pupil-teacher ratio by town 
- lstat: lower status of the population (percent) 
- medv: median value of owner-occupied homes in $1000s 
- target: whether the crime rate is above the median crime rate (1) or not (0)

## Assumptions for Logistical Regression

When we do logistical regression, we have to consider our assumptions and then verify our data holds up to them.
We must ensure that there is low or no multicollinearity between features.
Additionally, the features need to be linearly related.
Our minimum sample size at n% probability would be 10*13/n so our sample size is adequate (466 > 260) where 50% of samples are expected to be over the median by definition.
Finally, by phrasing the question appropriately, we can make a boolean outcome variable.
Going forward, for our research question we ask "is the crime present in this neighborhood above the median rate?"

```{r, include=FALSE, echo=FALSE}
dataUrl = "https://github.com/Anthogonyst/AnalyticsModeling/tree/master/data"

all_train_raw = LoadDataElseWeb("crime-training-data_modified.csv", dataUrl)
all_test = LoadDataElseWeb("crime-evaluation-data_modified.csv", dataUrl)
```

```{r, include=FALSE, echo=FALSE}
rename_crime <- function(df){
  df <- df %>% 
    dplyr::rename(
      `Large Lot Zoning` = zn,
      `Prop. Industrial Zoning` = indus,
      `Borders Charles` = chas,
      `Nitrous Oxide (pp million unit)` = nox, 
      `Avg Rooms per Dwelling` = rm,
      `Prop. Owner-Occupied Units Built Prior to 1940` = age, 
      `Weighted Mean of Distances to Five Boston Employment Centers` = dis,
      `Index of Accessibility to Radial Highways` = rad, 
      `Property-Tax Rate per $10k` = tax, 
      `Pupil-Teacher Ratio` = ptratio, 
      `Percent Pop. Lower SES` = lstat,
      `Median Value of Owner-Occupied Homes (in $1000s)` = medv, 
      `High Crime Status` = target
    )
  return(df)
}

glimpse(rename_crime(all_train_raw))
```

## Summary Statistics

Upon analyzing the target variable, we observe that 237 out of the total observations have a crime rate below the median, whereas 229 have a crime rate above the median. Consequently, our training data set comprises an almost equal number of neighborhoods categorized as at-risk and not-at-risk.

```{r}
summary(all_train_raw)
```

## Correlation

Our first curiosity is if there is any strong correlation to the target variable.
Accordingly, the nox, age, rad, tax, and indus variables show moderate positive correlation (>0.6).
Additionally, the dis variable shows moderate negative correlation (< -0.6).
Overall, nothing is too strongly correlated to be able to justify not building a model.

```{r}
cor_matrix <- cor(all_train_raw)
cor_with_target <- cor_matrix["target", colnames(cor_matrix) != "target"]
cor_with_target
```

However, for the big picture sense, we also need to verify that there is no multicollinearity.
According to our visualization, the variables rad and tax may be just too closely correlated together.
We may have to instead use PCA to create an orthogonal complement of those two variables instead.

```{r}
PlotCorrEllipse(cor_matrix)
```

## Data structure

There are 466 observations and 13 variables in the training dataset.

Additionally, there are no missing values so imputation is not necessary on this dataset.
Shown below is a sample of its head.

```{r}
str(all_train_raw)
```

## Missing values

The dataset is complete with no missing values.

```{r, include=FALSE, echo=FALSE}
nrow(all_train_raw[is.na(all_train_raw),])
```

## Visulization of the data set

First, let's examine what the data looks like without separating the target variables.
At first, we notice that indus and rad appears binomial.
Additionally, age and nox seem to be skewed in one direction.
Thankfully, we do not have to normalize the data until necessary because it is not an assumption for logistical regression.

```{r}
datasub = reshape2::melt(all_train_raw, id=c("target"))
ggplot2::ggplot(datasub) +
  ggplot2::aes(x = value) + 
  ggplot2::geom_density(fill = "skyblue") + 
  ggplot2::facet_wrap(~variable, scales = 'free') 
```

Next, let's examine the distribution of the features with respect to the target variable.
Noticably, some features strongly belong to one group such as indus whereas others are more ambiguous.
Therefore, we should expect all of the models to weigh indus heavily.
Additionally, we can expect other boundaries to get weighed with some significance such as nox and lstat.

```{r}
ggplot2::ggplot(datasub) +
  ggplot2::aes(x = "", y = value, group = target, fill = target) +
  ggplot2::geom_violin(adjust = 1L, scale = "area") +
  ggplot2::coord_flip() +
  ggplot2::theme_minimal() +
  ggplot2::facet_wrap(ggplot2::vars(variable), scales = "free")
```

# Data Preparation

As both "chas" and "target" are categorical variables, we have changed their class from integer to factor.

```{r}
all_train = all_train_raw
all_train$chas <- as.factor(all_train$chas)
all_train$target <- as.factor(all_train$target)
```

## Cross Validation

To verify our results, we will split the available labeled data into training and validation sets.
This will help us score our model on test data and to be able to verify the results later.

## Outliers

Based on the analysis, there were no outliers that needed to be removed as all the values appeared to be reasonable.

## Buckets

The code is using the summary information provided above to transform the data by putting it into "buckets" or categories. The variable cols_to_bin contains the names of the columns in the dataset that we want to bin, and breaks contains the breaks for the buckets we want to use for each variable.

```{r}

cols_to_bin <- c("zn", "indus", "nox", "rm", "age", "dis", "rad","tax", "ptratio", "lstat", "medv")

# create the breaks for the buckets
breaks <- list(zn = c(0, 20, 40, 60, 80, 100),
               indus = c(0, 5, 10, 15, 20, 30),
               nox = c(0, 0.4, 0.5, 0.6, 0.7, 0.9),
               rm = c(0, 4, 5, 6, 7, 8, 9),
               age = c(0, 20, 40, 60, 80, 100),
               dis = c(0, 2, 4, 6, 8, 10, 12),
               rad = c(0, 5, 10, 15, 20, 25, 30),
               tax = c(0, 250, 500, 750, 1000),
               ptratio = c(0, 14, 16, 18, 20, 22),
               lstat = c(0, 5, 10, 15, 20, 25, 30, 40),
               medv = c(0, 10, 20, 30, 40, 50))

# loop through columns, bin data, and print frequency table
for (col in cols_to_bin) {
  binned_col <- cut(all_train[[col]], breaks = breaks[[col]], include.lowest = TRUE)
  binned_col <- as.data.frame(binned_col) # convert factor to data frame
  colnames(binned_col) <- col # set the column name to the variable name
  print(table(binned_col))
}
```
## new variables 

**tax_per_room (tpr)** variable would represent the insights into the relationship between the cost of living in a particular area and the size of the living space. A higher value of **tax_per_room (tpr)** means that the tax rate is higher relative to the number of rooms in the dwelling.

**age_dis_ratio(adr)** can help identify areas that have an older housing inventory and are farther from job centers, which may affect preference for those locations.

```{r}
all_train <- all_train %>% 
             mutate(tpr = tax / rm,
                    adr = age / dis)
                    
head(all_train)
```

```{r}
train_clean_pre <- all_train 
set.seed(121)
split <- caret::createDataPartition(train_clean_pre$target, p=0.85, list=FALSE)
train_clean <- train_clean_pre[split, ]
validation <- train_clean_pre[ -split, ]
```

# Logistical Model Building

To begin, we will create a null model that does not make any prediction.
This will help us verify our first model works better than random guessing.
If we set our alpha value to 0.05 as common, the p value of almost 1 greatly exceeds it.
We hope to at least have a model with a p value below 0.05 to deem it statistically valuable.

```{r nullModel}
lmNull = rep(0, nrow(all_train["target"])) %>%
  { .[1] = 1 ; . } %>%
    as.factor(.) %>%
      cbind(., all_train["target"]) %>%
        magrittr::set_colnames(., c("X", "y")) %>%
          stats::glm(y ~ X, data = ., family = 'binomial')

summary.lm(lmNull)
```

## Model 1 - Full Model

We will start with a top-down approach and begin by including all of the variables.
According to this, our most statistically consistent variables are those with extremely low p-values.
Notably, the variables nox and rad both have very high significance bellow 0.001 so we will definitely use these in the future.

# TODO: Insert explanation about why this beats the null model

```{r}
modelOne <- stats::glm(target ~ ., data = train_clean, family = 'binomial')
summary(modelOne)
```

## Model 2 - P-values Selection

Before we begin stepwise addition and subtraction, let's also run a model with only variables with significance below 0.05 value.
This will give us a good baseline for later analysis.

```{r}
modelTwo <- stats::glm(target ~ nox + age + rad + tax + ptratio + lstat, 
               data = train_clean, 
               family = 'binomial')
summary(modelTwo)
```

## Model 3 - Backward Selection

Next, let's use the AIC scoring mechanism to do backwards stepwise addition.
This will give us more variables to compare as long as the threshold is below a significant point.

# TODO: Elaborate.

```{r}
modelThree <- MASS::stepAIC(modelTwo, direction = "backward", trace = FALSE)
summary(modelThree)
```

## Model 4 & 5 - Forward Selection

Finally, let's use that same AIC scoring mechanism to do forwards stepwise subtraction.
This will reduce some of the extra variables we added while still constraining ourselves to the most optimal fit.
Notably, when we run forward selection on our second model, it reaches a local maximum and stops.
However, if we run forwards AIC on model one, it reaches a different local maximum with a lower AIC than this one.

```{r}
modelFour <- MASS::stepAIC(modelTwo, direction = "forward", trace = FALSE)
summary(modelFour)
```

This fifth model, nicknamed 1-F, when compared to the previous model, 2-F, goes from AIC 210 to 201.
Therefore, it should hypothetically be better.
However, we will use hypothesis testing to accept or deny this model since we only want to change our model when it is statistically significant.

```{r}
modelFive <- MASS::stepAIC(modelOne, direction = "forward", trace = FALSE)
summary(modelFive)
```

# Model Selection

Our first model for consideration is the full model.

```{r, include=FALSE}
MakePredictions <- function(model, testData, excludeCol = "target", threshold = 0.5) {
  probability = stats::predict(model, testData[, ! colnames(testData) %in% excludeCol])
  class = probability %>%
    { .[. > threshold] = 1 ; . } %>%
      { .[. <= threshold] = 0 ; . } %>%
        as.factor(.)

  data.frame(class, probability)
}

VerifyKfold <- function(model, testData, excludeCol = "target", threshold = 0.5) {
  MakePredictions(model, testData, excludeCol, threshold) %>%
    { caret::confusionMatrix(.$class, testData[, colnames(testData) %in% excludeCol], mode = "everything") }
}

ValidationPipeline <- function(model, testData, excludeCol, plotname = "", threshold = 0.5) {
  p = MakePredictions(model, testData, excludeCol)
  d = VerifyKfold(model, testData, excludeCol)
  g = graphics::fourfoldplot(d$table, color = c("#B22222", "#2E8B57"), main = plotname)
  
  print(formula(model))
  print(d)
  print(g)
  
  invisible(list(p, d, g))
}

ValidationPipeline(modelOne, validation, "target", "Model 1")
```

```{r}
formula(modelOne)
predictions = MakePredictions(modelOne, validation, "target")
cv1 = VerifyKfold(modelOne, validation, "target")
cv1
graphics::fourfoldplot(cv1$table, color = c("#B22222", "#2E8B57"), main="Model 1")
```

Our second model for consideration is the manually selected p values model.

```{r}
formula(modelTwo)
cv2 = VerifyKfold(modelTwo, validation, "target")
cv2
graphics::fourfoldplot(cv2$table, color = c("#B22222", "#2E8B57"), main="Model 2")
```

Our third model is the AIC backwards selection model.

```{r}
formula(modelThree)
cv3 = VerifyKfold(modelThree, validation, "target")
cv3
graphics::fourfoldplot(cv3$table, color = c("#B22222", "#2E8B57"), main="Model 3")
```

Our fourth model is the AIC forwards selection model from model two, nicknamed 2-F in comparison to the next model.

```{r}
formula(modelFour)
cv4 = VerifyKfold(modelFour, validation, "target")
cv4
graphics::fourfoldplot(cv4$table, color = c("#B22222", "#2E8B57"), main = "Model 4")
```

Our fifth model is the AIC forwards selection model from model one, nicknamed 1-F in comparison to the previous model.

```{r}
formula(modelFive)
cv5 = VerifyKfold(modelFive, validation, "target")
cv5
graphics::fourfoldplot(cv5$table, color = c("#B22222", "#2E8B57"), main = "Model 5")
```


accuracy and lower error rate

```{r}
temp <- data.frame(cv1$overall,
                   cv2$overall,
                   cv3$overall,
                   cv4$overall,
                   cv5$overall) %>%
  t() %>%
  data.frame() %>%
  dplyr::select(Accuracy) %>%
  mutate(Classification_Error_Rate = 1 - Accuracy)
Summ_Stat <- data.frame(cv1$byClass,
                        cv2$byClass,
                        cv3$byClass,
                        cv4$byClass,
                        cv5$byClass) %>%
  t() %>%
  data.frame() %>%
  cbind(temp) %>%
  mutate(Model = c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5")) %>%
  dplyr::select(Model,
                Accuracy,
                Classification_Error_Rate,
                Precision,
                Sensitivity,
                Specificity,
                F1) %>%
  mutate_if(is.numeric, round, 3)# %>%
#kable('html', escape = F) %>%
#kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),full_width = F)
Summ_Stat
```

Model 2,3 has better AUC

```{r}
getROC <- function(model) {
    name <- deparse(substitute(model))
    pred.prob1 <- predict(model, newdata = validation)
    p1 <- data.frame(pred = validation$target, prob = pred.prob1)
    p1 <- p1[order(p1$prob),]
    rocobj <- pROC::roc(p1$pred, p1$prob, levels = c(0, 1), direction = "<")
    plot(rocobj, asp=NA, legacy.axes = TRUE, print.auc=TRUE,
         xlab="Specificity", main = name)
}
par(mfrow=c(2,2))
getROC(modelOne)
getROC(modelTwo)
getROC(modelThree)
getROC(modelFive)
```

Based on comparison result, we eventually choose Model 2/3 (same model)

Make Prediction

```{r}
all_test$chas <- as.factor(all_test$chas)
prediction = predict(modelThree, newdata = all_test)
prediction[prediction >= 0.5] <- 1
prediction[prediction < 0.5] <- 0
prediction = as.factor(prediction)
prediction
```

# Conclusions and Final Thoughts

None at all.

# References

Datasets are provided by CUNY School of Professional Studies for academic purposes.
It is reflective of public data gathered online.

# Appendix

Shown here is a copy of all relevant R code.

```{r, class.output="foldable"}
MakePredictions
```

```{r, class.output="foldable"}
VerifyKfold
```

```{r, class.output="foldable"}
getROC
```

```{r, class.output="foldable"}
PlotCorrEllipse
```
